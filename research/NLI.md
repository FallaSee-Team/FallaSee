# Natural Language Inference (NLI)

- NLI pays attention to the structure of the text
- Useful for unlabelled data


## Jin et al (2022):
Model consists of two „layers“

### 1) Structure-Aware Premise:
 - Mask out content words in the input text and output a logical form with placeholders
- Example: <br> 
<b>Input</b>: “Jack is a good athlete. Jack comes from Canada. Therefore, all Canadians are good athletes.”
<b>Output</b>: “[MSK1] is a [MSK2]. [MSK1] comes from [MSK3]. Therefore, all [MSK3] are [MSK2].”

How to create the layer?
1) Conduct coreference resolution using the CoreNLP package (Manning et al., 2014)
2) To identify word spans that are paraphrases of each other, they consider only nonstop words, lemmatize them via the Stanza package (Qi et al., 2020)

3) Represent each word by its contextualized embedding generated by Sentence-BERT (Reimers and Gurevych, 2019)

4) Calculate pair-wise cosine similarity. When the cosine similarity is larger than a threshold (by a grid search on the dev set), they identify the two words as similar. For each group i of similar text spans, we replace them with a mask token [MSKi].



<br>

## 2) Structure-Aware Hypothesis
- Uses logical form of the fallacies
- For instance, for circular reasoning it is often the form: ‘A is true because B is true; B is true because A is true.”
- The logical forms are compiled using the masking format


## Better alternatives?
Fine-tuning a transformer (e.g., DeBERTa, T5) on a labeled dataset will give better accuracy results than NLI