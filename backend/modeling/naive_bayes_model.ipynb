{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Basic Model: Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maren/Library/CloudStorage/OneDrive-PersoÃànlich/Dokumente/neue_fische/Capstone Project/Capstone_project/backend/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "\n",
    "import mlflow\n",
    "import logging \n",
    "import config \n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from basic_functions import (\n",
    "    get_preprocess_data,\n",
    "    get_lemmatized_data,\n",
    "    get_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"Naive_Bayes_oversampling\" \n",
    "TRACKING_URI = open(\"../.mlflow_uri\").read().strip()\n",
    "EXPERIMENT_NAME = config.EXPERIMENT_NAME\n",
    "\n",
    "logging.basicConfig(format=\"%(asctime)s: %(message)s\") # Configure logging format to show timestamp before every message\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO) # Only show logs that are INFO or more important (e.g., WARNING, ERROR) ‚Äî but ignore DEBUG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"../data/data_dropped_duplicates_small.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get and process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_preprocess_data(DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/maren/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_lemmatized_data(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[\"logical_fallacies\"]\n",
    "X = df[\"text\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.30, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(TRACKING_URI)\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "mlflow.start_run()\n",
    "run = mlflow.active_run()\n",
    "mlflow.set_tag(\"model_name\", MODEL_NAME)\n",
    "mlflow.set_tag(\"mlflow.runName\", \"naive bayes oversampling\")\n",
    "# mlflow.log_params(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline for TF-IFD and Naive Bayes\n",
    "pipeline_bayes = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('ros', RandomOverSampler(random_state=10)),\n",
    "    ('nb', MultinomialNB()),\n",
    "])\n",
    "\n",
    "# Train the model\n",
    "pipeline_bayes.fit(X_train, y_train)\n",
    "\n",
    "# Get predictions on the resampled training data\n",
    "X_res, y_res = pipeline_bayes.named_steps['ros'].fit_resample(\n",
    "    pipeline_bayes.named_steps['tfidf'].transform(X_train), \n",
    "    y_train\n",
    ")\n",
    "y_res_pred = pipeline_bayes.named_steps['nb'].predict(X_res)\n",
    "\n",
    "# Predict on test data\n",
    "y_test_pred_bayes = pipeline_bayes.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_metrics(cr, split):\n",
    "    for key, value in cr.items():\n",
    "        if (key == \"accuracy\"):\n",
    "                # print(f\"{split}_{key}\", round(value,2))\n",
    "                mlflow.log_metric(f\"{split}_{key}\", value)\n",
    "        else:\n",
    "            for metric in value:\n",
    "                mlflow.log_metric(f\"{split}_{key}_{metric}\", value.get(metric))\n",
    "                # print(f\"{split}_{key}_{metric}\", round(value.get(metri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:get_train_metrics\n",
      "INFO:basic_functions:classification_report\n",
      "INFO:basic_functions:confusion_matrix\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       precision    recall  f1-score   support\n",
      "\n",
      "           ad_hominem       0.93      0.91      0.92      3800\n",
      "  appeal_to_authority       0.84      0.98      0.90      3800\n",
      "    appeal_to_emotion       0.96      0.82      0.88      3800\n",
      "        false_dilemma       0.88      0.97      0.92      3800\n",
      "faulty_generalization       0.89      0.90      0.90      3800\n",
      "                 none       0.87      0.77      0.82      3800\n",
      "\n",
      "             accuracy                           0.89     22800\n",
      "            macro avg       0.90      0.89      0.89     22800\n",
      "         weighted avg       0.90      0.89      0.89     22800\n",
      "\n",
      "[[3473  118   41   35   51   82]\n",
      " [  12 3711   13   10   10   44]\n",
      " [ 107  226 3113  117  128  109]\n",
      " [  15    5    5 3691   25   59]\n",
      " [  24   78   14  104 3436  144]\n",
      " [  88  289   73  232  192 2926]]\n"
     ]
    }
   ],
   "source": [
    "logger.info('get_train_metrics')\n",
    "classification_report_train = get_metrics(y_res, y_res_pred)\n",
    "log_metrics(classification_report_train, \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:get_test_metrics\n",
      "INFO:basic_functions:classification_report\n",
      "INFO:basic_functions:confusion_matrix\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       precision    recall  f1-score   support\n",
      "\n",
      "           ad_hominem       0.53      0.48      0.50       333\n",
      "  appeal_to_authority       0.26      0.68      0.38       219\n",
      "    appeal_to_emotion       0.67      0.38      0.49       504\n",
      "        false_dilemma       0.42      0.72      0.53       290\n",
      "faulty_generalization       0.43      0.50      0.46       420\n",
      "                 none       0.80      0.62      0.70      1629\n",
      "\n",
      "             accuracy                           0.57      3395\n",
      "            macro avg       0.52      0.56      0.51      3395\n",
      "         weighted avg       0.64      0.57      0.59      3395\n",
      "\n",
      "[[ 160   41   27   23   40   42]\n",
      " [   8  149   10    9   10   33]\n",
      " [  42   95  193   47   72   55]\n",
      " [   2   13    2  208   16   49]\n",
      " [  27   51   21   43  209   69]\n",
      " [  64  218   34  161  136 1016]]\n"
     ]
    }
   ],
   "source": [
    "logger.info('get_test_metrics')\n",
    "classification_report_test = get_metrics(y_test, y_test_pred_bayes)\n",
    "log_metrics(classification_report_test, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run naive bayes oversampling at: http://127.0.0.1:5001/#/experiments/823412171152425451/runs/45ea536892f14b35977beac85102d9fe\n",
      "üß™ View experiment at: http://127.0.0.1:5001/#/experiments/823412171152425451\n"
     ]
    }
   ],
   "source": [
    "mlflow.end_run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
