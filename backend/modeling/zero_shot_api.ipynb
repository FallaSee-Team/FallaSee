{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero shot using an API (LLAMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from groq import Groq\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Either notebook requires credentials to be loaded from a `.env` file, which should contain respectively either of the following lines, if not both:\n",
    "```\n",
    "GROQ_API_KEY=<your groq api key>\n",
    "```\n",
    "\n",
    "- [Groq API Key](https://console.groq.com/playground) can be generated and used free of charge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/data_small.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>dataset</th>\n",
       "      <th>text</th>\n",
       "      <th>logical_fallacies</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18384</td>\n",
       "      <td>8</td>\n",
       "      <td>Testing on animals could save the life of you ...</td>\n",
       "      <td>appeal_to_emotion</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11271</td>\n",
       "      <td>3</td>\n",
       "      <td>I remember when China took over Hong Kong, I r...</td>\n",
       "      <td>none</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15702</td>\n",
       "      <td>4</td>\n",
       "      <td>: The only \"Light at the End of the Tunnel\", i...</td>\n",
       "      <td>appeal_to_emotion</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7148</td>\n",
       "      <td>3</td>\n",
       "      <td>So you only believe there are two ways to run ...</td>\n",
       "      <td>none</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8147</td>\n",
       "      <td>3</td>\n",
       "      <td>Keep things the way they are or change them co...</td>\n",
       "      <td>false_dilemma</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  dataset                                               text   \n",
       "0       18384        8  Testing on animals could save the life of you ...  \\\n",
       "1       11271        3  I remember when China took over Hong Kong, I r...   \n",
       "2       15702        4  : The only \"Light at the End of the Tunnel\", i...   \n",
       "3        7148        3  So you only believe there are two ways to run ...   \n",
       "4        8147        3  Keep things the way they are or change them co...   \n",
       "\n",
       "   logical_fallacies source  \n",
       "0  appeal_to_emotion    NaN  \n",
       "1               none    NaN  \n",
       "2  appeal_to_emotion    NaN  \n",
       "3               none    NaN  \n",
       "4      false_dilemma    NaN  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dotenv with API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv('../../.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate API key\n",
    "assert os.environ.get(\"GROQ_API_KEY\"), \"GROQ_API_KEY not found in .env file\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "#calling llm from platform, llama3 is called the llm\n",
    "#temperature is hyperparameter, how creative do I want the llm to be (0 is not creative), sometimes, when it is not 0, it can give you the second likely word\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model=\"llama3-8b-8192\",  # Replace with your desired Llama model version\n",
    "    temperature=0,           # No randomness, deterministic output\n",
    "    max_tokens=None,         # Unlimited token length (adjust as needed)\n",
    "    # timeout=None,            # No timeout (can be adjusted)\n",
    "    max_retries=2            # Retry twice on transient failures\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_fallacy(text: str) -> str:\n",
    "    \"\"\"Classifies text into one of the predefined logical fallacies.\"\"\"\n",
    "    try:\n",
    "        # Combine system instructions and user content in one message\n",
    "        # This avoids system message compatibility issues\n",
    "        prompt = \"\"\"Classify the following text into exactly one logical fallacy category:\n",
    "- faulty_generalization\n",
    "- ad_hominem\n",
    "- false_dilemma \n",
    "- appeal_to_authority\n",
    "- appeal_to_emotion  \n",
    "- none\n",
    "\n",
    "Text to classify: {0}\n",
    "\n",
    "Respond ONLY with the category name and nothing else.\"\"\".format(text)\n",
    "\n",
    "        # Simplified message structure - only user message\n",
    "        response = llm.invoke(prompt)\n",
    "        \n",
    "        # Properly extract content based on LangChain's response structure\n",
    "        prediction = response.content.strip().lower()\n",
    "        \n",
    "        # Normalize the response\n",
    "        valid_categories = [\"faulty_generalization\", \"ad_hominem\", \"false_dilemma\", \n",
    "                          \"appeal_to_authority\", \"appeal_to_emotion\", \"none\"]\n",
    "        \n",
    "        # Match to valid categories\n",
    "        for category in valid_categories:\n",
    "            if category in prediction:\n",
    "                return category\n",
    "                \n",
    "        return prediction if prediction in valid_categories else \"none\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing text: {text[:50]}... | Error: {str(e)}\")\n",
    "        return \"Error\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataframe(df: pd.DataFrame, batch_size=10) -> pd.DataFrame:\n",
    "    \"\"\"Process DataFrame with chunking for better performance.\"\"\"\n",
    "    result_df = df.copy()\n",
    "    \n",
    "    # Process in smaller batches to reduce API errors\n",
    "    chunks = [df[i:i+batch_size] for i in range(0, len(df), batch_size)]\n",
    "    \n",
    "    with tqdm(total=len(df), desc=\"Classifying Logical Fallacies\") as pbar:\n",
    "        for chunk in chunks:\n",
    "            # Process each text in the chunk\n",
    "            chunk_results = []\n",
    "            for text in chunk['text']:\n",
    "                result = classify_fallacy(text)\n",
    "                chunk_results.append(result)\n",
    "                pbar.update(1)\n",
    "                \n",
    "            # Update results for this chunk\n",
    "            result_df.loc[chunk.index, 'predicted_fallacy'] = chunk_results\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions only based on 1000 rows\n",
    "df_small = df.iloc[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying Logical Fallacies: 100%|██████████| 1000/1000 [38:18<00:00,  2.30s/it]\n"
     ]
    }
   ],
   "source": [
    "# Process the DataFrame and classify logical fallacies\n",
    "processed_df = process_dataframe(df_small)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>dataset</th>\n",
       "      <th>text</th>\n",
       "      <th>logical_fallacies</th>\n",
       "      <th>source</th>\n",
       "      <th>predicted_fallacy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18384</td>\n",
       "      <td>8</td>\n",
       "      <td>Testing on animals could save the life of you ...</td>\n",
       "      <td>appeal_to_emotion</td>\n",
       "      <td>NaN</td>\n",
       "      <td>faulty_generalization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11271</td>\n",
       "      <td>3</td>\n",
       "      <td>I remember when China took over Hong Kong, I r...</td>\n",
       "      <td>none</td>\n",
       "      <td>NaN</td>\n",
       "      <td>faulty_generalization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15702</td>\n",
       "      <td>4</td>\n",
       "      <td>: The only \"Light at the End of the Tunnel\", i...</td>\n",
       "      <td>appeal_to_emotion</td>\n",
       "      <td>NaN</td>\n",
       "      <td>faulty_generalization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7148</td>\n",
       "      <td>3</td>\n",
       "      <td>So you only believe there are two ways to run ...</td>\n",
       "      <td>none</td>\n",
       "      <td>NaN</td>\n",
       "      <td>false_dilemma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8147</td>\n",
       "      <td>3</td>\n",
       "      <td>Keep things the way they are or change them co...</td>\n",
       "      <td>false_dilemma</td>\n",
       "      <td>NaN</td>\n",
       "      <td>false_dilemma</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  dataset                                               text   \n",
       "0       18384        8  Testing on animals could save the life of you ...  \\\n",
       "1       11271        3  I remember when China took over Hong Kong, I r...   \n",
       "2       15702        4  : The only \"Light at the End of the Tunnel\", i...   \n",
       "3        7148        3  So you only believe there are two ways to run ...   \n",
       "4        8147        3  Keep things the way they are or change them co...   \n",
       "\n",
       "   logical_fallacies source      predicted_fallacy  \n",
       "0  appeal_to_emotion    NaN  faulty_generalization  \n",
       "1               none    NaN  faulty_generalization  \n",
       "2  appeal_to_emotion    NaN  faulty_generalization  \n",
       "3               none    NaN          false_dilemma  \n",
       "4      false_dilemma    NaN          false_dilemma  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       precision    recall  f1-score   support\n",
      "\n",
      "           ad_hominem       0.72      0.56      0.63       101\n",
      "  appeal_to_authority       0.81      0.46      0.59        56\n",
      "    appeal_to_emotion       0.24      0.08      0.12       155\n",
      "        false_dilemma       0.82      0.52      0.64        86\n",
      "faulty_generalization       0.16      0.92      0.27       133\n",
      "                 none       0.63      0.03      0.05       469\n",
      "\n",
      "             accuracy                           0.27      1000\n",
      "            macro avg       0.56      0.43      0.38      1000\n",
      "         weighted avg       0.54      0.27      0.23      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(processed_df[\"logical_fallacies\"], processed_df[\"predicted_fallacy\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
