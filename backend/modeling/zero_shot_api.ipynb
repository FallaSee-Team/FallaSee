{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero shot using an API (LLAMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from groq import Groq\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Either notebook requires credentials to be loaded from a `.env` file, which should contain respectively either of the following lines, if not both:\n",
    "```\n",
    "GROQ_API_KEY=<your groq api key>\n",
    "```\n",
    "\n",
    "- [Groq API Key](https://console.groq.com/playground) can be generated and used free of charge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/data_small.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>dataset</th>\n",
       "      <th>text</th>\n",
       "      <th>logical_fallacies</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18384</td>\n",
       "      <td>8</td>\n",
       "      <td>Testing on animals could save the life of you ...</td>\n",
       "      <td>appeal_to_emotion</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11271</td>\n",
       "      <td>3</td>\n",
       "      <td>I remember when China took over Hong Kong, I r...</td>\n",
       "      <td>none</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15702</td>\n",
       "      <td>4</td>\n",
       "      <td>: The only \"Light at the End of the Tunnel\", i...</td>\n",
       "      <td>appeal_to_emotion</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7148</td>\n",
       "      <td>3</td>\n",
       "      <td>So you only believe there are two ways to run ...</td>\n",
       "      <td>none</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8147</td>\n",
       "      <td>3</td>\n",
       "      <td>Keep things the way they are or change them co...</td>\n",
       "      <td>false_dilemma</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  dataset                                               text   \n",
       "0       18384        8  Testing on animals could save the life of you ...  \\\n",
       "1       11271        3  I remember when China took over Hong Kong, I r...   \n",
       "2       15702        4  : The only \"Light at the End of the Tunnel\", i...   \n",
       "3        7148        3  So you only believe there are two ways to run ...   \n",
       "4        8147        3  Keep things the way they are or change them co...   \n",
       "\n",
       "   logical_fallacies source  \n",
       "0  appeal_to_emotion    NaN  \n",
       "1               none    NaN  \n",
       "2  appeal_to_emotion    NaN  \n",
       "3               none    NaN  \n",
       "4      false_dilemma    NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dotenv with API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv('../../.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate API key\n",
    "assert os.environ.get(\"GROQ_API_KEY\"), \"GROQ_API_KEY not found in .env file\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "#calling llm from platform, llama3 is called the llm\n",
    "#temperature is hyperparameter, how creative do I want the llm to be (0 is not creative), sometimes, when it is not 0, it can give you the second likely word\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model=\"llama3-8b-8192\",  # Replace with your desired Llama model version\n",
    "    temperature=0,           # No randomness, deterministic output\n",
    "    max_tokens=None,         # Unlimited token length (adjust as needed)\n",
    "    # timeout=None,            # No timeout (can be adjusted)\n",
    "    max_retries=2            # Retry twice on transient failures\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'appeal_to_emotion': \"When I was in a rope line just the other day, coming out here from Wisconsin, a couple of young returnees were in the line, one active duty, one from the Guard. And they both looked at me and said: We need you. You've got to help us over there.\",\n",
       " 'none': ': MORE VACCINE GOOD NEWS ü•≤ Moderna: Covid vaccine shows nearly 95% protection',\n",
       " 'false_dilemma': \"If we don't take this risk, we'll regret it forever.\",\n",
       " 'ad_hominem': ':  SO MANY DIED! JOE IS THE WORST! MAGA ü§∞',\n",
       " 'faulty_generalization': \"This fallacy suggests that unlikely or ridiculous outcomes are likely when there's just not enough evidence to think so.\",\n",
       " 'appeal_to_authority': \"And if you support them, don't be surprised that that is exactly what they want to do and that is why Dan Quayle refuses to say this evening that he supports the right of a woman to choose.\"}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to get a random text for each unique logical fallacy\n",
    "def get_random_text_per_fallacy(df):\n",
    "    result = {}\n",
    "    unique_fallacies = df['logical_fallacies'].unique()\n",
    "    for fallacy in unique_fallacies:\n",
    "        random_text = df[df['logical_fallacies'] == fallacy].sample(n=1)['text'].iloc[0]\n",
    "        result[fallacy] = random_text\n",
    "    return result\n",
    "\n",
    "# Get random text examples for each fallacy\n",
    "random_texts = get_random_text_per_fallacy(df)\n",
    "\n",
    "# Print the result\n",
    "random_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_fallacy(text: str) -> str:\n",
    "    \"\"\"Classifies text into one of the predefined logical fallacies.\"\"\"\n",
    "    try:\n",
    "        # Combine system instructions and user content in one message\n",
    "        # This avoids system message compatibility issues\n",
    "        prompt = \"\"\"Classify the following text into exactly one logical fallacy category:\n",
    "- faulty_generalization\n",
    "- ad_hominem\n",
    "- false_dilemma \n",
    "- appeal_to_authority\n",
    "- appeal_to_emotion  \n",
    "- none\n",
    "\n",
    "Here are definitions of each category for reference:\n",
    "1. **Faulty Generalization**: This fallacy occurs when an argument assumes something is true for a large population without having a large enough sample. A kind of overgeneralization.\n",
    "2. **Ad Hominem**: This fallacy occurs when the speaker is attacking the other person or some aspect of them rather than addressing the argument itself.\n",
    "3. **False Dilemma**: This fallacy occurs when only two options are presented in an argument, even though more options may exist. A case of ‚Äúeither this or that‚Äù.\n",
    "4. **Appeal to Authority**: This fallacy occurs when an argument relies on the opinion or endorsement of an authority figure who may not have relevant expertise or whose expertise is questionable. When applicable, a scientific consensus is not an appeal to authority.\n",
    "5. **Appeal to Emotion**: This fallacy occurs when emotion is used to support an argument, such as pity, fear, anger, etc.\n",
    "6. **None**: There are no fallacies in this text!\n",
    "\n",
    "Here are examples of each category for reference:\n",
    "1. **Faulty Generalization**: \"I read one report about corruption, so that industry must be corrupt.\"\n",
    "2. **Ad Hominem**: \"Do you even know what you're talking about?\"\n",
    "3. **False Dilemma**: \"Do you recommend drinking or injecting bleach to fight Covid?\"\n",
    "4. **Appeal to Authority**: \"Trust me, I am a lawyer, so I know how to handle your taxes.\"\n",
    "5. **Appeal to Emotion**: \"You murdered 100,000 people, called Coronavirus a hoax, fired doctors, and told Americans to inject themselves with bleach. Maybe you should shut the fuck up.\"\n",
    "\n",
    "Text to classify: {0}\n",
    "\n",
    "Respond ONLY with the category name and nothing else.\"\"\".format(text)\n",
    "\n",
    "        # Simplified message structure - only user message\n",
    "        response = llm.invoke(prompt)\n",
    "        \n",
    "        # Properly extract content based on LangChain's response structure\n",
    "        prediction = response.content.strip().lower()\n",
    "        \n",
    "        # Normalize the response\n",
    "        valid_categories = [\"faulty_generalization\", \"ad_hominem\", \"false_dilemma\", \n",
    "                          \"appeal_to_authority\", \"appeal_to_emotion\", \"none\"]\n",
    "        \n",
    "        # Match to valid categories\n",
    "        for category in valid_categories:\n",
    "            if category in prediction:\n",
    "                return category\n",
    "                \n",
    "        return prediction if prediction in valid_categories else \"none\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing text: {text[:50]}... | Error: {str(e)}\")\n",
    "        return \"Error\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataframe(df: pd.DataFrame, batch_size=10) -> pd.DataFrame:\n",
    "    \"\"\"Process DataFrame with chunking for better performance.\"\"\"\n",
    "    result_df = df.copy()\n",
    "    \n",
    "    # Process in smaller batches to reduce API errors\n",
    "    chunks = [df[i:i+batch_size] for i in range(0, len(df), batch_size)]\n",
    "    \n",
    "    with tqdm(total=len(df), desc=\"Classifying Logical Fallacies\") as pbar:\n",
    "        for chunk in chunks:\n",
    "            # Process each text in the chunk\n",
    "            chunk_results = []\n",
    "            for text in chunk['text']:\n",
    "                result = classify_fallacy(text)\n",
    "                chunk_results.append(result)\n",
    "                pbar.update(1)\n",
    "                \n",
    "            # Update results for this chunk\n",
    "            result_df.loc[chunk.index, 'predicted_fallacy'] = chunk_results\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions only based on 1000 rows\n",
    "df_small = df.iloc[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying Logical Fallacies:  28%|‚ñà‚ñà‚ñä       | 279/1000 [21:02<54:22,  4.53s/it]  "
     ]
    }
   ],
   "source": [
    "# Process the DataFrame and classify logical fallacies\n",
    "processed_df = process_dataframe(df_small)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>dataset</th>\n",
       "      <th>text</th>\n",
       "      <th>logical_fallacies</th>\n",
       "      <th>source</th>\n",
       "      <th>predicted_fallacy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18384</td>\n",
       "      <td>8</td>\n",
       "      <td>Testing on animals could save the life of you ...</td>\n",
       "      <td>appeal_to_emotion</td>\n",
       "      <td>NaN</td>\n",
       "      <td>faulty_generalization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11271</td>\n",
       "      <td>3</td>\n",
       "      <td>I remember when China took over Hong Kong, I r...</td>\n",
       "      <td>none</td>\n",
       "      <td>NaN</td>\n",
       "      <td>faulty_generalization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15702</td>\n",
       "      <td>4</td>\n",
       "      <td>: The only \"Light at the End of the Tunnel\", i...</td>\n",
       "      <td>appeal_to_emotion</td>\n",
       "      <td>NaN</td>\n",
       "      <td>faulty_generalization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7148</td>\n",
       "      <td>3</td>\n",
       "      <td>So you only believe there are two ways to run ...</td>\n",
       "      <td>none</td>\n",
       "      <td>NaN</td>\n",
       "      <td>false_dilemma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8147</td>\n",
       "      <td>3</td>\n",
       "      <td>Keep things the way they are or change them co...</td>\n",
       "      <td>false_dilemma</td>\n",
       "      <td>NaN</td>\n",
       "      <td>false_dilemma</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  dataset                                               text   \n",
       "0       18384        8  Testing on animals could save the life of you ...  \\\n",
       "1       11271        3  I remember when China took over Hong Kong, I r...   \n",
       "2       15702        4  : The only \"Light at the End of the Tunnel\", i...   \n",
       "3        7148        3  So you only believe there are two ways to run ...   \n",
       "4        8147        3  Keep things the way they are or change them co...   \n",
       "\n",
       "   logical_fallacies source      predicted_fallacy  \n",
       "0  appeal_to_emotion    NaN  faulty_generalization  \n",
       "1               none    NaN  faulty_generalization  \n",
       "2  appeal_to_emotion    NaN  faulty_generalization  \n",
       "3               none    NaN          false_dilemma  \n",
       "4      false_dilemma    NaN          false_dilemma  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       precision    recall  f1-score   support\n",
      "\n",
      "           ad_hominem       0.72      0.56      0.63       101\n",
      "  appeal_to_authority       0.81      0.46      0.59        56\n",
      "    appeal_to_emotion       0.24      0.08      0.12       155\n",
      "        false_dilemma       0.82      0.52      0.64        86\n",
      "faulty_generalization       0.16      0.92      0.27       133\n",
      "                 none       0.63      0.03      0.05       469\n",
      "\n",
      "             accuracy                           0.27      1000\n",
      "            macro avg       0.56      0.43      0.38      1000\n",
      "         weighted avg       0.54      0.27      0.23      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(processed_df[\"logical_fallacies\"], processed_df[\"predicted_fallacy\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
