{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero shot using LLAMA locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before coding, you need to do this in your terminal:\n",
    "1. brew install ollama (install ollama framework for starting LLMs locally)\n",
    "2. pip install langchain-ollama (in virtual environment)\n",
    "2. ollama serve (start ollama server)\n",
    "3. get model name from: https://ollama.com/library/\n",
    "4. ollama run llama3.1:8b (in second terminal; downloads model and runs it)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'lor_model' from 'mlflow.transformers' (/Users/maren/Library/CloudStorage/OneDrive-Persönlich/Dokumente/neue_fische/Capstone Project/Capstone_project/backend/.venv/lib/python3.11/site-packages/mlflow/transformers/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmlflow\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmlflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msklearn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m save_model\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmlflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m lor_model\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlogging\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mconfic\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'lor_model' from 'mlflow.transformers' (/Users/maren/Library/CloudStorage/OneDrive-Persönlich/Dokumente/neue_fische/Capstone Project/Capstone_project/backend/.venv/lib/python3.11/site-packages/mlflow/transformers/__init__.py)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, classification_report\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from langchain_ollama import OllamaLLM\n",
    "\n",
    "import mlflow\n",
    "from mlflow.sklearn import save_model\n",
    "from mlflow.transformers import lor_model\n",
    "import logging\n",
    "\n",
    "import confic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLFlow setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"llama3.1\" \n",
    "TRACKING_URI = open(\"../.mlflow_uri\").read().strip()\n",
    "EXPERIMENT_NAME = config.EXPERIMENT_NAME\n",
    "\n",
    "logging.basicConfig(format=\"%(asctime)s: %(message)s\") # Configure logging format to show timestamp before every message\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO) # Only show logs that are INFO or more important (e.g., WARNING, ERROR) — but ignore DEBUG."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/data_small.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>dataset</th>\n",
       "      <th>text</th>\n",
       "      <th>logical_fallacies</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18384</td>\n",
       "      <td>8</td>\n",
       "      <td>Testing on animals could save the life of you ...</td>\n",
       "      <td>appeal_to_emotion</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11271</td>\n",
       "      <td>3</td>\n",
       "      <td>I remember when China took over Hong Kong, I r...</td>\n",
       "      <td>none</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15702</td>\n",
       "      <td>4</td>\n",
       "      <td>: The only \"Light at the End of the Tunnel\", i...</td>\n",
       "      <td>appeal_to_emotion</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7148</td>\n",
       "      <td>3</td>\n",
       "      <td>So you only believe there are two ways to run ...</td>\n",
       "      <td>none</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8147</td>\n",
       "      <td>3</td>\n",
       "      <td>Keep things the way they are or change them co...</td>\n",
       "      <td>false_dilemma</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  dataset                                               text   \n",
       "0       18384        8  Testing on animals could save the life of you ...  \\\n",
       "1       11271        3  I remember when China took over Hong Kong, I r...   \n",
       "2       15702        4  : The only \"Light at the End of the Tunnel\", i...   \n",
       "3        7148        3  So you only believe there are two ways to run ...   \n",
       "4        8147        3  Keep things the way they are or change them co...   \n",
       "\n",
       "   logical_fallacies source  \n",
       "0  appeal_to_emotion    NaN  \n",
       "1               none    NaN  \n",
       "2  appeal_to_emotion    NaN  \n",
       "3               none    NaN  \n",
       "4      false_dilemma    NaN  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize local Llama model using Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "#temperature is hyperparameter, how creative do I want the llm to be (0 is not creative), sometimes, when it is not 0, it can give you the second likely word\n",
    "\n",
    "# Initialize the local Llama model using Ollama\n",
    "llm = OllamaLLM(\n",
    "    model=\"llama3.1:8b\",  # Replace with your desired local Llama model version\n",
    "    temperature=0,        # No randomness, deterministic output\n",
    "    max_tokens=None,      # Unlimited token length (adjust as needed)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_fallacy(text: str) -> str:\n",
    "    \"\"\"Classifies text into one of the predefined logical fallacies.\"\"\"\n",
    "    try:\n",
    "        prompt = f\"\"\"Classify the following text into exactly one logical fallacy category:\n",
    "- faulty_generalization\n",
    "- ad_hominem\n",
    "- false_dilemma \n",
    "- appeal_to_authority\n",
    "- appeal_to_emotion  \n",
    "- none\n",
    "\n",
    "Here are definitions of each category for reference:\n",
    "1. **Faulty Generalization**: This fallacy occurs when an argument assumes something is true for a large population without having a large enough sample. A kind of overgeneralization.\n",
    "2. **Ad Hominem**: This fallacy occurs when the speaker is attacking the other person or some aspect of them rather than addressing the argument itself.\n",
    "3. **False Dilemma**: This fallacy occurs when only two options are presented in an argument, even though more options may exist. A case of “either this or that”.\n",
    "4. **Appeal to Authority**: This fallacy occurs when an argument relies on the opinion or endorsement of an authority figure who may not have relevant expertise or whose expertise is questionable.\n",
    "5. **Appeal to Emotion**: This fallacy occurs when emotion is used to support an argument, such as pity, fear, anger, etc.\n",
    "6. **None**: There are no fallacies in this text!\n",
    "\n",
    "Here are examples of each category for reference:\n",
    "1. **Faulty Generalization**: \"I read one report about corruption, so that industry must be corrupt.\"\n",
    "2. **Ad Hominem**: \"Do you even know what you're talking about?\"\n",
    "3. **False Dilemma**: \"Do you recommend drinking or injecting bleach to fight Covid?\"\n",
    "4. **Appeal to Authority**: \"Trust me, I am a lawyer, so I know how to handle your taxes.\"\n",
    "5. **Appeal to Emotion**: \"You murdered 100,000 people, called Coronavirus a hoax, fired doctors, and told Americans to inject themselves with bleach. Maybe you should shut the fuck up.\"\n",
    "6. **None**: \"I don't think that kind of logic is good. It's essentially saying that so long as the authoritarians repress their people enough and nobody can rise up against them, we should think things are okay. We should strive to ensure they have leaders who value democracy, not oppression and authoritarianism.\"\n",
    "\n",
    "Text to classify: {text}\n",
    "\n",
    "Respond ONLY with the category name and nothing else.\"\"\"\n",
    "\n",
    "        # Generate response using the local Llama model\n",
    "        response = llm.invoke(prompt)\n",
    "        \n",
    "        # Extract and normalize response content\n",
    "        prediction = response.strip().lower()\n",
    "        valid_categories = [\"faulty_generalization\", \"ad_hominem\", \"false_dilemma\", \n",
    "                            \"appeal_to_authority\", \"appeal_to_emotion\", \"none\"]\n",
    "        \n",
    "        return prediction if prediction in valid_categories else \"none\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing text: {text[:50]}... | Error: {str(e)}\")\n",
    "        return \"Error\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataframe(df: pd.DataFrame, batch_size=10) -> pd.DataFrame:\n",
    "    \"\"\"Process DataFrame with chunking for better performance.\"\"\"\n",
    "    result_df = df.copy()\n",
    "    \n",
    "    # Process in smaller batches to reduce errors\n",
    "    chunks = [df[i:i+batch_size] for i in range(0, len(df), batch_size)]\n",
    "    \n",
    "    with tqdm(total=len(df), desc=\"Classifying Logical Fallacies\") as pbar:\n",
    "        for chunk in chunks:\n",
    "            chunk_results = []\n",
    "            for text in chunk['text']:\n",
    "                result = classify_fallacy(text)\n",
    "                chunk_results.append(result)\n",
    "                pbar.update(1)\n",
    "                \n",
    "            # Update results for this chunk\n",
    "            result_df.loc[chunk.index, 'predicted_fallacy'] = chunk_results\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions only based on 1000 rows\n",
    "df_small = df.iloc[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying Logical Fallacies: 100%|██████████| 1000/1000 [10:01<00:00,  1.66it/s]\n"
     ]
    }
   ],
   "source": [
    "# Process the DataFrame and classify logical fallacies\n",
    "processed_df = process_dataframe(df_small)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>dataset</th>\n",
       "      <th>text</th>\n",
       "      <th>logical_fallacies</th>\n",
       "      <th>source</th>\n",
       "      <th>predicted_fallacy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18384</td>\n",
       "      <td>8</td>\n",
       "      <td>Testing on animals could save the life of you ...</td>\n",
       "      <td>appeal_to_emotion</td>\n",
       "      <td>NaN</td>\n",
       "      <td>faulty_generalization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11271</td>\n",
       "      <td>3</td>\n",
       "      <td>I remember when China took over Hong Kong, I r...</td>\n",
       "      <td>none</td>\n",
       "      <td>NaN</td>\n",
       "      <td>faulty_generalization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15702</td>\n",
       "      <td>4</td>\n",
       "      <td>: The only \"Light at the End of the Tunnel\", i...</td>\n",
       "      <td>appeal_to_emotion</td>\n",
       "      <td>NaN</td>\n",
       "      <td>faulty_generalization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7148</td>\n",
       "      <td>3</td>\n",
       "      <td>So you only believe there are two ways to run ...</td>\n",
       "      <td>none</td>\n",
       "      <td>NaN</td>\n",
       "      <td>false_dilemma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8147</td>\n",
       "      <td>3</td>\n",
       "      <td>Keep things the way they are or change them co...</td>\n",
       "      <td>false_dilemma</td>\n",
       "      <td>NaN</td>\n",
       "      <td>false_dilemma</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  dataset                                               text   \n",
       "0       18384        8  Testing on animals could save the life of you ...  \\\n",
       "1       11271        3  I remember when China took over Hong Kong, I r...   \n",
       "2       15702        4  : The only \"Light at the End of the Tunnel\", i...   \n",
       "3        7148        3  So you only believe there are two ways to run ...   \n",
       "4        8147        3  Keep things the way they are or change them co...   \n",
       "\n",
       "   logical_fallacies source      predicted_fallacy  \n",
       "0  appeal_to_emotion    NaN  faulty_generalization  \n",
       "1               none    NaN  faulty_generalization  \n",
       "2  appeal_to_emotion    NaN  faulty_generalization  \n",
       "3               none    NaN          false_dilemma  \n",
       "4      false_dilemma    NaN          false_dilemma  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       precision    recall  f1-score   support\n",
      "\n",
      "           ad_hominem       0.86      0.38      0.52       101\n",
      "  appeal_to_authority       0.57      0.54      0.55        56\n",
      "    appeal_to_emotion       0.33      0.15      0.20       155\n",
      "        false_dilemma       0.83      0.50      0.62        86\n",
      "faulty_generalization       0.18      0.93      0.30       133\n",
      "                 none       0.63      0.10      0.18       469\n",
      "\n",
      "             accuracy                           0.31      1000\n",
      "            macro avg       0.56      0.43      0.40      1000\n",
      "         weighted avg       0.56      0.31      0.29      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(processed_df[\"logical_fallacies\"], processed_df[\"predicted_fallacy\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
