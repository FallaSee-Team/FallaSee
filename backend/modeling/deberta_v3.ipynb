{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeBERTa-v3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maren/Library/CloudStorage/OneDrive-Persönlich/Dokumente/neue_fische/Capstone Project/Capstone_project/backend/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from mlflow.sklearn import save_model\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import sentencepiece\n",
    "import os\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "from transformers import TrainingArguments, Trainer, AutoModelForSequenceClassification, AutoModel, AutoTokenizer, AutoConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLFlow setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"deberta_v3\" \n",
    "DATA_PATH = \"../data/data_small.csv\"\n",
    "TRACKING_URI = config.TRACKING_URI #??? TRACKING_URI = open(\"../.mlflow_uri\").read().strip()\n",
    "EXPERIMENT_NAME = config.EXPERIMENT_NAME\n",
    "\n",
    "logging.basicConfig(format=\"%(asctime)s: %(message)s\") # Configure logging format to show timestamp before every message\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO) # Only show logs that are INFO or more important (e.g., WARNING, ERROR) — but ignore DEBUG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/data_small.csv')\n",
    "\n",
    "Y = df[\"logical_fallacies\"]\n",
    "X = df[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5000 entries, 0 to 4999\n",
      "Data columns (total 5 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   Unnamed: 0         5000 non-null   int64 \n",
      " 1   dataset            5000 non-null   int64 \n",
      " 2   text               5000 non-null   object\n",
      " 3   logical_fallacies  5000 non-null   object\n",
      " 4   source             240 non-null    object\n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 195.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, Y, test_size=0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/76868251/how-to-load-deberta-v3-properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenization after train test split to prevent data leakage\n",
    "\n",
    "#added use_fast=False to prevent tokenization error (might happen when using fast tokenization)\n",
    "tokenizer = AutoTokenizer.from_pretrained('microsoft/deberta-v3-base', use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(texts):\n",
    "    return tokenizer(\n",
    "        texts,\n",
    "        padding=\"max_length\", #ensures that all tokenized sequences are padded to the same length, padding adds special tokens to shorter sequeces so they match the maximum length\n",
    "        truncation=True, #if sequence exceeds max, it will be trucated\n",
    "        max_length=512, #for most transformer models, 512 is a common limit for maximum length\n",
    "        return_tensors=\"pt\" #converts the output to pytorch tensors\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenize(X_train.to_list())\n",
    "test_encodings = tokenize(X_test.to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert string labels to integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train) \n",
    "y_test = le.transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Preparation for usage in model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "needed to create a PyTorch Dataset object that:\n",
    "- organizes tokenized text\n",
    "- pairs them with corresponding labels\n",
    "- structures everything for batch processing during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#object oriented programming (class is the object), with class you can do different things, such as calling functions\n",
    "\n",
    "class TextDataset(Dataset):  # Inherits from PyTorch's Dataset class\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.input_ids = encodings['input_ids']       # Token IDs from tokenizer\n",
    "        self.attention_mask = encodings['attention_mask']  # Mask for padding\n",
    "        self.labels = torch.tensor(labels)  # Convert labels to tensors\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.input_ids[idx],       # Token IDs for one sample\n",
    "            'attention_mask': self.attention_mask[idx],  # Mask for one sample\n",
    "            'labels': self.labels[idx]              # Label for one sample\n",
    "        }\n",
    "    def __len__(self):\n",
    "        return len(self.labels)  # Total number of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TextDataset(train_encodings, y_train)\n",
    "test_dataset = TextDataset(test_encodings, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero Shot Inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # disable upper limit for memory\n",
    "# os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.0\"\n",
    "\n",
    "# # Allows up to 100% of available memory\n",
    "# torch.mps.set_per_process_memory_fraction(1.0)  \n",
    "\n",
    "# torch.mps.empty_cache()  # Clears unused GPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load fresh copy of base model (not train on our data)\n",
    "# num_classes = len(df[\"logical_fallacies\"].unique())\n",
    "# base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "#     \"microsoft/deberta-v3-small\",\n",
    "#     num_labels=num_classes,\n",
    "#     problem_type=\"single_label_classification\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predict(model, encodings, batch_size=8):\n",
    "#     # Set the model to evaluation mode\n",
    "#     model.eval()\n",
    "    \n",
    "#     # Use GPU\n",
    "#     device = torch.device(\"mps\")\n",
    "#     model.to(device)\n",
    "    \n",
    "#     # Perform inference\n",
    "#     probabilities = []\n",
    "#     for i in range(0, len(encodings[\"input_ids\"]), batch_size):\n",
    "#         with torch.no_grad():\n",
    "#             batch = {\n",
    "#                 \"input_ids\": encodings[\"input_ids\"][i:i+batch_size].to(device),\n",
    "#                 \"attention_mask\": encodings[\"attention_mask\"][i:i+batch_size].to(device)\n",
    "#             }\n",
    "#             outputs = model(**batch)\n",
    "#             probs = torch.softmax(outputs.logits, dim=-1).cpu().numpy()\n",
    "#             probabilities.extend(probs)\n",
    "            \n",
    "#         # Clear GPU memory after each batch\n",
    "#         torch.mps.empty_cache()\n",
    "    \n",
    "#     return np.array(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get predictions for test data\n",
    "# base_probs = predict(base_model, test_encodings, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get highest probability indices\n",
    "# predicted_indices = np.argmax(base_probs, axis=1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import classification_report\n",
    "\n",
    "# # Generate classification report\n",
    "# report = classification_report(y_test, predicted_indices, target_names=le.classes_)\n",
    "# print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: This deberta model is actually not designed for zero shot, there is one by MoritzLauer which can be used without requiring training on data. So training on data is actually necessary! The DeBERTa used here is meant for supervised learning. \n",
    "Another option is to use BART, facebook/bart-large-mnli model.\n",
    "\n",
    "**Zero-Shot Learning** </span> is a concept, that a model when trained on enough unlabeled data (unsupervised learning) is able to generalize/ recognize at inference time even though the model was not trained on the inference data. This can be used in NLP, Images etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I had to change configuration of accelerate, as it might still be configured to fp16 (mixed precision)(doesn't work on Apple M1 Pro):\n",
    "- type in bash accelerate confic\n",
    "- this machine\n",
    "- no distributed training\n",
    "- do you want to run your training on CPU only, say No, as MAC Apple M1 Pro has GPU\n",
    "- do you wish to optimize script with torch dynamo: say \"No\" if using an Apple M1 Pro with MPS backend\n",
    "- do you want to use mixed prexision: NO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "num_classes = len(df[\"logical_fallacies\"].unique())\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"microsoft/deberta-v3-small\",\n",
    "    num_labels=num_classes,\n",
    "    problem_type=\"single_label_classification\"\n",
    ")\n",
    "\n",
    "model.gradient_checkpointing_enable()  # force model to use gradient checkpointing to save memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class-balanced trainer\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert class weights to tensor\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class WeightedLossTrainer(Trainer):\n",
    "    def __init__(self, class_weights=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.class_weights = class_weights\n",
    "        if self.class_weights is not None:\n",
    "            # Move weights to device after model initialization\n",
    "            self._move_weights_to_device()\n",
    "    \n",
    "    def _move_weights_to_device(self):\n",
    "        self.class_weights = self.class_weights.to(self.model.device)\n",
    "\n",
    "    def compute_loss(\n",
    "        self, \n",
    "        model, \n",
    "        inputs, \n",
    "        return_outputs=False, \n",
    "        num_items_in_batch=None  # Add this parameter\n",
    "    ):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        loss_fct = nn.CrossEntropyLoss(weight=self.class_weights)\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), \n",
    "                       labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Confguration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='../models/LLM_deberta_v3_small_class_imbalance/trainer_output', # to sve results\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4, #small to save memory\n",
    "    per_device_eval_batch_size=8, #small to save memory\n",
    "    learning_rate=2e-5, #standard for deberta; maybe try 6e-6\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_steps=50,\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    preds = p.predictions.argmax(-1)\n",
    "    return {'accuracy': accuracy_score(p.label_ids, preds)}\n",
    "\n",
    "trainer = WeightedLossTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    class_weights=class_weights\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mps.empty_cache()  # Clears unused GPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disable upper limit for memory\n",
    "os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.0\"\n",
    "\n",
    "# Allows up to 100% of available memory\n",
    "torch.mps.set_per_process_memory_fraction(1.0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2625' max='2625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2625/2625 33:22, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.991800</td>\n",
       "      <td>0.982793</td>\n",
       "      <td>0.641333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.833200</td>\n",
       "      <td>1.239198</td>\n",
       "      <td>0.740000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.658600</td>\n",
       "      <td>1.400739</td>\n",
       "      <td>0.751333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2625, training_loss=0.9162530321393694, metrics={'train_runtime': 2004.204, 'train_samples_per_second': 5.239, 'train_steps_per_second': 1.31, 'total_flos': 1391056450560000.0, 'train_loss': 0.9162530321393694, 'epoch': 3.0})"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.6413\n"
     ]
    }
   ],
   "source": [
    "results = trainer.evaluate()\n",
    "print(f\"Test Accuracy: {results['eval_accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = trainer.predict(test_dataset)\n",
    "predictions = np.argmax(output.predictions, axis=1)\n",
    "y_true = output.label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       precision    recall  f1-score   support\n",
      "\n",
      "           ad_hominem       0.62      0.68      0.65       159\n",
      "  appeal_to_authority       0.42      0.74      0.54        97\n",
      "    appeal_to_emotion       0.80      0.56      0.66       227\n",
      "        false_dilemma       0.75      0.65      0.69       133\n",
      "faulty_generalization       0.41      0.73      0.52       202\n",
      "                 none       0.82      0.62      0.70       682\n",
      "\n",
      "             accuracy                           0.64      1500\n",
      "            macro avg       0.63      0.66      0.63      1500\n",
      "         weighted avg       0.70      0.64      0.65      1500\n",
      "\n",
      "Confusion Matrix:\n",
      "[[108  10  15   0  10  16]\n",
      " [  6  72   3   2   6   8]\n",
      " [ 27   4 128   3  42  23]\n",
      " [  5   2   4  86  13  23]\n",
      " [ 11  12   4   3 147  25]\n",
      " [ 18  71   7  21 144 421]]\n"
     ]
    }
   ],
   "source": [
    "# Generate classification report\n",
    "print(classification_report(y_true, predictions, target_names=le.classes_))\n",
    "\n",
    "# Generate confusion matrix\n",
    "cm = confusion_matrix(y_true, predictions)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I saved the model here as a SK model, although it is a Pytorch model. Keep that in mind!\n",
    "\n",
    "Save model:\n",
    "import mlflow.pytorch\n",
    "mlflow.pytorch.save_model(model, \"deberta_model\")\n",
    "\n",
    "Load model (correct way):\n",
    "model = mlflow.pytorch.load_model(\"deberta_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/03 14:03:26 WARNING mlflow.sklearn: Model was missing function: predict. Not logging python_function flavor!\n"
     ]
    }
   ],
   "source": [
    "#save with sklearn\n",
    "path_sk = \"../models/LLM_deberta_v3_small_class_imbalance/sk_learn_model\"\n",
    "save_model(sk_model=model, path=path_sk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save with pytorch\n",
    "path_pt = \"../models/LLM_deberta_v3_small_class_imbalance/pytorch_model\"\n",
    "import mlflow.pytorch\n",
    "mlflow.pytorch.save_model(model, path=path_pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow.sklearn\n",
    "path_sk = \"../models/LLM_deberta_v3_small_class_imbalance/sk_learn_model\"\n",
    "model = mlflow.sklearn.load_model(path_sk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow.pytorch\n",
    "path_pt = \"../models/LLM_deberta_v3_small_class_imbalance/pytorch_model\"\n",
    "model = mlflow.pytorch.load_model(path_pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions based on reloaded model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the code works, although I loaded it as a Sklearn model, because I manually converted the logits to probabilites with torch.softmax. \n",
    "\n",
    "mlflow.sklearn.load_model() accidentally worked because MLflow can sometimes load PyTorch models as generic Python objects, but this isn't reliable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # batch can be changed both codes, now used the upper one\n",
    "# # upper one makes it more generalized for dynamic inputs, as the lower one only handles input_ids and attention_mask\n",
    "# batch = {\n",
    "#     key: val[i:i+batch_size].to(device) \n",
    "#     for key, val in encodings.items()\n",
    "# }\n",
    "\n",
    "# batch = {\n",
    "#                 \"input_ids\": encodings[\"input_ids\"][i:i+batch_size].to(device),\n",
    "#                 \"attention_mask\": encodings[\"attention_mask\"][i:i+batch_size].to(device)\n",
    "#             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function for prediction\n",
    "\n",
    "def predict(model, encodings, batch_size=8):\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Use GPU\n",
    "    device = torch.device(\"mps\")\n",
    "    model.to(device)\n",
    "    \n",
    "    # Perform inference\n",
    "    probabilities = []\n",
    "    for i in range(0, len(encodings[\"input_ids\"]), batch_size):\n",
    "        with torch.no_grad():\n",
    "            batch = {\n",
    "                key: val[i:i+batch_size].to(device) \n",
    "                for key, val in encodings.items()\n",
    "            }\n",
    "            outputs = model(**batch)\n",
    "            probs = torch.softmax(outputs.logits, dim=-1).cpu().numpy()\n",
    "            probabilities.extend(probs)\n",
    "            \n",
    "        # Clear GPU memory after each batch\n",
    "        torch.mps.empty_cache()\n",
    "    \n",
    "    return np.array(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#needed to reduce the batch size, otherwise I had an error\n",
    "# Get predictions for test data\n",
    "base_probs = predict(model, test_encodings, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get highest probability indices\n",
    "predicted_labels = np.argmax(base_probs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get second highest probability indices\n",
    "second_predicted_labels = np.argsort(base_probs, axis=1)[:, -2]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#probabilites of first predicted\n",
    "predicted_label_probs = base_probs[np.arange(len(predicted_labels)), predicted_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#probabilites of second predicted\n",
    "second_predicted_label_probs = np.sort(base_probs, axis=1)[:, -2]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for backend \n",
    "result = {\n",
    "    \"predicted_labels\": predicted_labels,\n",
    "    \"predicted_label_probs\": predicted_label_probs,\n",
    "    \"second_predicted_labels\": second_predicted_labels,\n",
    "    \"second_predicted_label_probs\": second_predicted_label_probs\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       precision    recall  f1-score   support\n",
      "\n",
      "           ad_hominem       0.62      0.68      0.65       159\n",
      "  appeal_to_authority       0.42      0.74      0.54        97\n",
      "    appeal_to_emotion       0.80      0.56      0.66       227\n",
      "        false_dilemma       0.75      0.65      0.69       133\n",
      "faulty_generalization       0.41      0.73      0.52       202\n",
      "                 none       0.82      0.62      0.70       682\n",
      "\n",
      "             accuracy                           0.64      1500\n",
      "            macro avg       0.63      0.66      0.63      1500\n",
      "         weighted avg       0.70      0.64      0.65      1500\n",
      "\n",
      "Confusion Matrix:\n",
      "[[108  10  15   0  10  16]\n",
      " [  6  72   3   2   6   8]\n",
      " [ 27   4 128   3  42  23]\n",
      " [  5   2   4  86  13  23]\n",
      " [ 11  12   4   3 147  25]\n",
      " [ 18  71   7  21 144 421]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(y_test, predicted_labels, target_names=le.classes_)\n",
    "print(report)\n",
    "\n",
    "# Generate confusion matrix\n",
    "cm = confusion_matrix(y_test, predicted_labels)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiclass Brier score: 0.49395134797722307\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "# 1. One-hot encode the true labels (y_test)\n",
    "lb = LabelBinarizer()\n",
    "y_true_onehot = lb.fit_transform(y_test)  # Shape: (n_samples, n_classes)\n",
    "\n",
    "# 2. Compute Brier score for multiclass\n",
    "brier_score = np.mean(np.sum((base_probs - y_true_onehot) ** 2, axis=1))\n",
    "print(\"Multiclass Brier score:\", brier_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
