{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeBERTa-v3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maren/Library/CloudStorage/OneDrive-Persönlich/Dokumente/neue_fische/Capstone Project/Capstone_project/backend/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import mlflow\n",
    "from mlflow.transformers import log_model\n",
    "import logging \n",
    "from mlflow.sklearn import save_model\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from mlflow.models.signature import infer_signature\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from scipy.special import softmax\n",
    "import numpy as np\n",
    "\n",
    "from torch import nn\n",
    "import mlflow.pytorch\n",
    "\n",
    "import sentencepiece\n",
    "import os\n",
    "\n",
    "# os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # This tells Hugging Face: “Don’t use parallel tokenization — avoid possible deadlocks.”\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "from transformers import TrainingArguments, Trainer, AutoModelForSequenceClassification, AutoModel, AutoTokenizer, AutoConfig\n",
    "\n",
    "import config "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from basic_functions import(\n",
    "    get_encode_tokenize_data,\n",
    "    createTrainer,\n",
    "    get_eval_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"deberta_v3\" \n",
    "TRACKING_URI = open(\"../.mlflow_uri\").read().strip()\n",
    "EXPERIMENT_NAME = config.EXPERIMENT_NAME\n",
    "\n",
    "logging.basicConfig(format=\"%(asctime)s: %(message)s\") # Configure logging format to show timestamp before every message\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO) # Only show logs that are INFO or more important (e.g., WARNING, ERROR) — but ignore DEBUG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"../data/data_dropped_duplicates_small.csv\"\n",
    "MODEL_PATH = \"microsoft/deberta-v3-base\"\n",
    "MODEL_TRAINING_PATH =\"microsoft/deberta-v3-small\"\n",
    "OUTPUT_DIR = \"../models/LLM_deberta_v3_tiny/trainer_output\"\n",
    "SAVE_PATH = \"../models/LLM_deberta_v3_tiny/pytorch_model\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:basic_functions:Loading data...\n",
      "INFO:basic_functions:Train test split, test-size 0.3\n",
      "INFO:root:encode the label column\n",
      "INFO:root:tokenize\n",
      "INFO:basic_functions:create tokenizer & load model\n",
      "INFO:basic_functions:create tokenizer & load model\n",
      "INFO:root:create TextDatasets (train & test)\n"
     ]
    }
   ],
   "source": [
    "train_dataset, test_dataset, y_train, le = get_encode_tokenize_data(DATA_PATH, MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero Shot Inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # disable upper limit for memory\n",
    "# os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.0\"\n",
    "\n",
    "# # Allows up to 100% of available memory\n",
    "# torch.mps.set_per_process_memory_fraction(1.0)  \n",
    "\n",
    "# torch.mps.empty_cache()  # Clears unused GPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load fresh copy of base model (not train on our data)\n",
    "# num_classes = len(df[\"logical_fallacies\"].unique())\n",
    "# base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "#     \"microsoft/deberta-v3-small\",\n",
    "#     num_labels=num_classes,\n",
    "#     problem_type=\"single_label_classification\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predict(model, encodings, batch_size=8):\n",
    "#     # Set the model to evaluation mode\n",
    "#     model.eval()\n",
    "    \n",
    "#     # Use GPU\n",
    "#     device = torch.device(\"mps\")\n",
    "#     model.to(device)\n",
    "    \n",
    "#     # Perform inference\n",
    "#     probabilities = []\n",
    "#     for i in range(0, len(encodings[\"input_ids\"]), batch_size):\n",
    "#         with torch.no_grad():\n",
    "#             batch = {\n",
    "#                 \"input_ids\": encodings[\"input_ids\"][i:i+batch_size].to(device),\n",
    "#                 \"attention_mask\": encodings[\"attention_mask\"][i:i+batch_size].to(device)\n",
    "#             }\n",
    "#             outputs = model(**batch)\n",
    "#             probs = torch.softmax(outputs.logits, dim=-1).cpu().numpy()\n",
    "#             probabilities.extend(probs)\n",
    "            \n",
    "#         # Clear GPU memory after each batch\n",
    "#         torch.mps.empty_cache()\n",
    "    \n",
    "#     return np.array(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get predictions for test data\n",
    "# base_probs = predict(base_model, test_encodings, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get highest probability indices\n",
    "# predicted_indices = np.argmax(base_probs, axis=1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import classification_report\n",
    "\n",
    "# # Generate classification report\n",
    "# report = classification_report(y_test, predicted_indices, target_names=le.classes_)\n",
    "# print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: This deberta model is actually not designed for zero shot, there is one by MoritzLauer which can be used without requiring training on data. So training on data is actually necessary! The DeBERTa used here is meant for supervised learning. \n",
    "Another option is to use BART, facebook/bart-large-mnli model.\n",
    "\n",
    "**Zero-Shot Learning** </span> is a concept, that a model when trained on enough unlabeled data (unsupervised learning) is able to generalize/ recognize at inference time even though the model was not trained on the inference data. This can be used in NLP, Images etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I had to change configuration of accelerate, as it might still be configured to fp16 (mixed precision)(doesn't work on Apple M1 Pro):\n",
    "- type in bash accelerate config\n",
    "- this machine\n",
    "- no distributed training\n",
    "- do you want to run your training on CPU only, say No, as MAC Apple M1 Pro has GPU\n",
    "- do you wish to optimize script with torch dynamo: say \"No\" if using an Apple M1 Pro with MPS backend\n",
    "- do you want to use mixed precision: NO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = len(np.unique(y_train))\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_TRAINING_PATH,\n",
    "    num_labels=num_classes,\n",
    "    problem_type=\"single_label_classification\"\n",
    ")\n",
    "\n",
    "model.gradient_checkpointing_enable()  # force model to use gradient checkpointing to save memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch = 3\n",
    "# learning_rate=2e-5 #standard for deberta; maybe try 6e-6\n",
    "# weight_decay=0.01\n",
    "# per_device_train_batch_size=4 #small to save memory\n",
    "# per_device_eval_batch_size=8 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Active run_id: a982d0f5999943aaba97157c28e37303\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "      \"learning_rate\": 2e-5,\n",
    "      \"weight_decay\": 0.01,\n",
    "      \"num_train_epochs\": 1,\n",
    "      \"evaluation_strategy\": \"epoch\",\n",
    "      \"train_batch_size\":4, \n",
    "      \"eval_batch_size\":8,\n",
    "      \"epochs\": 4,\n",
    "      \"class_weight\": \"False\",\n",
    "  }\n",
    "  \n",
    "# setting the MLFlow connection and experiment\n",
    "mlflow.set_tracking_uri(TRACKING_URI)\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "mlflow.start_run()\n",
    "run = mlflow.active_run()\n",
    "print(\"Active run_id: {}\".format(run.info.run_id))\n",
    "\n",
    "mlflow.set_tag(\"model_name\", MODEL_NAME)\n",
    "mlflow.set_tag('mlflow.runName', 'deberta_v3_small')\n",
    "mlflow.log_params(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:defining training arguments\n",
      "INFO:root:get normal trainer\n"
     ]
    }
   ],
   "source": [
    "trainer = createTrainer(\n",
    "    model= model, \n",
    "    train_dataset = train_dataset,\n",
    "    test_dataset = test_dataset,\n",
    "    output_dir= OUTPUT_DIR, \n",
    "    y_train=y_train, \n",
    "    class_weight=False, \n",
    "    epochs=4, \n",
    "    learning_rate=2e-5, #standard for deberta; maybe try 6e-6\n",
    "    weight_decay = 0.01, \n",
    "    train_batch_size=4, \n",
    "    eval_batch_size=8 )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mps.empty_cache()  # Clears unused GPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disable upper limit for memory\n",
    "os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.0\"\n",
    "\n",
    "# Allows up to 100% of available memory\n",
    "torch.mps.set_per_process_memory_fraction(1.0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:training is running\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3500' max='3500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3500/3500 57:38, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.073000</td>\n",
       "      <td>1.031240</td>\n",
       "      <td>0.651333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.678000</td>\n",
       "      <td>0.955212</td>\n",
       "      <td>0.747333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.511200</td>\n",
       "      <td>1.077594</td>\n",
       "      <td>0.763333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.406800</td>\n",
       "      <td>1.148954</td>\n",
       "      <td>0.768667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/07 10:19:09 ERROR mlflow.utils.async_logging.async_logging_queue: Run Id a982d0f5999943aaba97157c28e37303: Failed to log run data: Exception: API request to http://127.0.0.1:5001/api/2.0/mlflow/runs/log-batch failed with exception HTTPConnectionPool(host='127.0.0.1', port=5001): Max retries exceeded with url: /api/2.0/mlflow/runs/log-batch (Caused by ResponseError('too many 500 error responses'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3500, training_loss=0.7577310218811035, metrics={'train_runtime': 3461.6108, 'train_samples_per_second': 4.044, 'train_steps_per_second': 1.011, 'total_flos': 1854741934080000.0, 'train_loss': 0.7577310218811035, 'epoch': 4.0})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logger.info('training is running')\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_metrics(cr, brier, split):\n",
    "    mlflow.log_metric(f\"{split}_brier\", brier)\n",
    "\n",
    "    for key, value in cr.items():\n",
    "        if (key == \"accuracy\"):\n",
    "                # print(f\"{split}_{key}\", round(value,2))\n",
    "                mlflow.log_metric(f\"{split}_{key}\", value)\n",
    "        else:\n",
    "            for metric in value:\n",
    "                mlflow.log_metric(f\"{split}_{key}_{metric}\", value.get(metric))\n",
    "                # print(f\"{split}_{key}_{metric}\", round(value.get(metric),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:predict on train_dataset\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:basic_functions:get evaluation metrics\n",
      "INFO:basic_functions:classification_report\n",
      "INFO:basic_functions:confusion_matrix\n",
      "INFO:basic_functions:brier score\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       precision    recall  f1-score   support\n",
      "\n",
      "           ad_hominem       0.89      0.69      0.78       331\n",
      "  appeal_to_authority       0.89      0.57      0.69       227\n",
      "    appeal_to_emotion       0.79      0.86      0.82       504\n",
      "        false_dilemma       0.91      0.70      0.79       319\n",
      "faulty_generalization       0.81      0.67      0.73       449\n",
      "                 none       0.85      0.98      0.91      1670\n",
      "\n",
      "             accuracy                           0.84      3500\n",
      "            macro avg       0.86      0.74      0.79      3500\n",
      "         weighted avg       0.85      0.84      0.84      3500\n",
      "\n",
      "[[ 228    5   49    1   13   35]\n",
      " [   3  129   30    5   18   42]\n",
      " [   8    1  434    3   16   42]\n",
      " [  11    0   10  223   16   59]\n",
      " [   6    1   19    3  302  118]\n",
      " [   1    9    7    9    8 1636]]\n",
      "Multiclass Brier score: 0.24648120722082073\n"
     ]
    }
   ],
   "source": [
    "logger.info('predict on train_dataset')\n",
    "train_output = trainer.predict(train_dataset)\n",
    "\n",
    "classification_report, brier= get_eval_metrics(train_output, le)\n",
    "log_metrics(classification_report, brier, \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:predict on test_dataset\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:basic_functions:get evaluation metrics\n",
      "INFO:basic_functions:classification_report\n",
      "INFO:basic_functions:confusion_matrix\n",
      "INFO:basic_functions:brier score\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       precision    recall  f1-score   support\n",
      "\n",
      "           ad_hominem       0.75      0.55      0.63       142\n",
      "  appeal_to_authority       0.80      0.37      0.51        97\n",
      "    appeal_to_emotion       0.67      0.78      0.72       216\n",
      "        false_dilemma       0.85      0.67      0.75       137\n",
      "faulty_generalization       0.63      0.46      0.53       192\n",
      "                 none       0.77      0.92      0.84       716\n",
      "\n",
      "             accuracy                           0.75      1500\n",
      "            macro avg       0.75      0.63      0.66      1500\n",
      "         weighted avg       0.75      0.75      0.73      1500\n",
      "\n",
      "[[ 78   1  28   0  11  24]\n",
      " [  7  36   9   1   7  37]\n",
      " [  6   2 168   2  10  28]\n",
      " [  2   0   2  92   7  34]\n",
      " [  4   0  27   1  89  71]\n",
      " [  7   6  16  12  17 658]]\n",
      "Multiclass Brier score: 0.4123306847059788\n"
     ]
    }
   ],
   "source": [
    "logger.info('predict on test_dataset')\n",
    "test_output = trainer.predict(test_dataset)\n",
    "\n",
    "classification_report, brier = get_eval_metrics(test_output, le)\n",
    "log_metrics(classification_report, brier, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run deberta_v3_small at: http://127.0.0.1:5001/#/experiments/823412171152425451/runs/a982d0f5999943aaba97157c28e37303\n",
      "🧪 View experiment at: http://127.0.0.1:5001/#/experiments/823412171152425451\n"
     ]
    }
   ],
   "source": [
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save with pytorch\n",
    "mlflow.pytorch.save_model(model, path=SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/07 11:23:51 WARNING mlflow.pytorch: Stored model version '2.6.0' does not match installed PyTorch version '2.2.2'\n"
     ]
    }
   ],
   "source": [
    "import mlflow.pytorch\n",
    "path_pt = \"../models/LLM_deberta_v3_small_class_imbalance/pytorch_model\"\n",
    "model = mlflow.pytorch.load_model(path_pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions based on reloaded model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function for prediction\n",
    "\n",
    "def predict(model, encodings, batch_size=8):\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Use GPU\n",
    "    device = torch.device(\"mps\")\n",
    "    model.to(device)\n",
    "    \n",
    "    # Perform inference\n",
    "    probabilities = []\n",
    "    for i in range(0, len(encodings[\"input_ids\"]), batch_size):\n",
    "        with torch.no_grad():\n",
    "            batch = {\n",
    "                key: val[i:i+batch_size].to(device) \n",
    "                for key, val in encodings.items()\n",
    "            }\n",
    "            outputs = model(**batch)\n",
    "            probs = torch.softmax(outputs.logits, dim=-1).cpu().numpy()\n",
    "            probabilities.extend(probs)\n",
    "            \n",
    "        # Clear GPU memory after each batch\n",
    "        torch.mps.empty_cache()\n",
    "    \n",
    "    return np.array(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#needed to reduce the batch size, otherwise I had an error\n",
    "# Get predictions for test data\n",
    "base_probs = predict(model, test_encodings, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get highest probability indices\n",
    "predicted_labels = np.argmax(base_probs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get second highest probability indices\n",
    "second_predicted_labels = np.argsort(base_probs, axis=1)[:, -2]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#probabilites of first predicted\n",
    "predicted_label_probs = base_probs[np.arange(len(predicted_labels)), predicted_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#probabilites of second predicted\n",
    "second_predicted_label_probs = np.sort(base_probs, axis=1)[:, -2]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for backend \n",
    "result = {\n",
    "    \"predicted_labels\": predicted_labels,\n",
    "    \"predicted_label_probs\": predicted_label_probs,\n",
    "    \"second_predicted_labels\": second_predicted_labels,\n",
    "    \"second_predicted_label_probs\": second_predicted_label_probs\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(y_test, predicted_labels, target_names=le.classes_)\n",
    "print(report)\n",
    "\n",
    "# Generate confusion matrix\n",
    "cm = confusion_matrix(y_test, predicted_labels)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "# # 1. One-hot encode the true labels (y_test)\n",
    "# lb = LabelBinarizer()\n",
    "# y_true_onehot = lb.fit_transform(y_test)  # Shape: (n_samples, n_classes)\n",
    "\n",
    "# # 2. Compute Brier score for multiclass\n",
    "# brier_score = np.mean(np.sum((base_probs - y_true_onehot) ** 2, axis=1))\n",
    "# print(\"Multiclass Brier score:\", brier_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
