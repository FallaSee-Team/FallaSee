{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeBERTa-v3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from mlflow.sklearn import save_model\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import sentencepiece\n",
    "import os\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "from transformers import TrainingArguments, Trainer, AutoModelForSequenceClassification, AutoModel, AutoTokenizer, AutoConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install requirements in terminal:\n",
    "pip install transformers torch datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/data_small.csv')\n",
    "\n",
    "Y = df[\"logical_fallacies\"]\n",
    "X = df[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5000 entries, 0 to 4999\n",
      "Data columns (total 5 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   Unnamed: 0         5000 non-null   int64 \n",
      " 1   dataset            5000 non-null   int64 \n",
      " 2   text               5000 non-null   object\n",
      " 3   logical_fallacies  5000 non-null   object\n",
      " 4   source             240 non-null    object\n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 195.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, Y, test_size=0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/76868251/how-to-load-deberta-v3-properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenization after train test split to prevent data leakage\n",
    "\n",
    "#added use_fast=False to prevent tokenization error (might happen when using fast tokenization)\n",
    "tokenizer = AutoTokenizer.from_pretrained('microsoft/deberta-v3-base', use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(texts):\n",
    "    return tokenizer(\n",
    "        texts,\n",
    "        padding=\"max_length\", #ensures that all tokenized sequences are padded to the same length, padding adds special tokens to shorter sequeces so they match the maximum length\n",
    "        truncation=True, #if sequence exceeds max, it will be trucated\n",
    "        max_length=512, #for most transformer models, 512 is a common limit for maximum length\n",
    "        return_tensors=\"pt\" #converts the output to pytorch tensors\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenize(X_train.to_list())\n",
    "test_encodings = tokenize(X_test.to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert string labels to integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train) \n",
    "y_test = le.transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Preparation for usage in model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "needed to create a PyTorch Dataset object that:\n",
    "- organizes tokenized text\n",
    "- pairs them with corresponding labels\n",
    "- structures everything for batch processing during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#object oriented programming (class is the object), with class you can do different things, such as calling functions\n",
    "\n",
    "class TextDataset(Dataset):  # Inherits from PyTorch's Dataset class\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.input_ids = encodings['input_ids']       # Token IDs from tokenizer\n",
    "        self.attention_mask = encodings['attention_mask']  # Mask for padding\n",
    "        self.labels = torch.tensor(labels)  # Convert labels to tensors\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.input_ids[idx],       # Token IDs for one sample\n",
    "            'attention_mask': self.attention_mask[idx],  # Mask for one sample\n",
    "            'labels': self.labels[idx]              # Label for one sample\n",
    "        }\n",
    "    def __len__(self):\n",
    "        return len(self.labels)  # Total number of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TextDataset(train_encodings, y_train)\n",
    "test_dataset = TextDataset(test_encodings, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero Shot Inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disable upper limit for memory\n",
    "os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.0\"\n",
    "\n",
    "# Allows up to 100% of available memory\n",
    "torch.mps.set_per_process_memory_fraction(1.0)  \n",
    "\n",
    "torch.mps.empty_cache()  # Clears unused GPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load fresh copy of base model (not train on our data)\n",
    "num_classes = len(df[\"logical_fallacies\"].unique())\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"microsoft/deberta-v3-small\",\n",
    "    num_labels=num_classes,\n",
    "    problem_type=\"single_label_classification\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, encodings, batch_size=8):\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Use GPU\n",
    "    device = torch.device(\"mps\")\n",
    "    model.to(device)\n",
    "    \n",
    "    # Perform inference\n",
    "    probabilities = []\n",
    "    for i in range(0, len(encodings[\"input_ids\"]), batch_size):\n",
    "        with torch.no_grad():\n",
    "            batch = {\n",
    "                \"input_ids\": encodings[\"input_ids\"][i:i+batch_size].to(device),\n",
    "                \"attention_mask\": encodings[\"attention_mask\"][i:i+batch_size].to(device)\n",
    "            }\n",
    "            outputs = model(**batch)\n",
    "            probs = torch.softmax(outputs.logits, dim=-1).cpu().numpy()\n",
    "            probabilities.extend(probs)\n",
    "            \n",
    "        # Clear GPU memory after each batch\n",
    "        torch.mps.empty_cache()\n",
    "    \n",
    "    return np.array(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions for test data\n",
    "base_probs = predict(base_model, test_encodings, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get highest probability indices\n",
    "predicted_indices = np.argmax(base_probs, axis=1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       precision    recall  f1-score   support\n",
      "\n",
      "           ad_hominem       0.00      0.00      0.00       159\n",
      "  appeal_to_authority       0.00      0.00      0.00        97\n",
      "    appeal_to_emotion       0.00      0.00      0.00       227\n",
      "        false_dilemma       0.09      1.00      0.16       133\n",
      "faulty_generalization       0.00      0.00      0.00       202\n",
      "                 none       0.00      0.00      0.00       682\n",
      "\n",
      "             accuracy                           0.09      1500\n",
      "            macro avg       0.01      0.17      0.03      1500\n",
      "         weighted avg       0.01      0.09      0.01      1500\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maren/Library/CloudStorage/OneDrive-Persönlich/Dokumente/neue_fische/Capstone Project/Capstone_project/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/maren/Library/CloudStorage/OneDrive-Persönlich/Dokumente/neue_fische/Capstone Project/Capstone_project/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/maren/Library/CloudStorage/OneDrive-Persönlich/Dokumente/neue_fische/Capstone Project/Capstone_project/backend/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(y_test, predicted_indices, target_names=le.classes_)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: This deberta model is actually not designed for zero shot, there is one by MoritzLauer which can be used without requiring training on data. So training on data is actually necessary! The DeBERTa used here is meant for supervised learning. \n",
    "Another option is to use BART, facebook/bart-large-mnli model.\n",
    "\n",
    "**Zero-Shot Learning** </span> is a concept, that a model when trained on enough unlabeled data (unsupervised learning) is able to generalize/ recognize at inference time even though the model was not trained on the inference data. This can be used in NLP, Images etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I had to change configuration of accelerate, as it might still be configured to fp16 (mixed precision):\n",
    "- type in bash accelerate confic\n",
    "- this machine\n",
    "- no distributed training\n",
    "- do you want to run your training on CPU only, say No, as MAC has GPU\n",
    "- do you wish to optimize script with torch dynamo: say \"No\" if using an Apple M1 Pro with MPS backend\n",
    "- do you want to use mixed prexision: NO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "num_classes = len(df[\"logical_fallacies\"].unique())\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"microsoft/deberta-v3-small\",\n",
    "    num_labels=num_classes,\n",
    "    problem_type=\"single_label_classification\"\n",
    ")\n",
    "\n",
    "model.gradient_checkpointing_enable()  # force model to use gradient checkpointing to save memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class imbalance (We could try)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Class-balanced trainer\n",
    "# from sklearn.utils.class_weight import compute_class_weight\n",
    "# class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Confguration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    # output_dir='./results', # to sve results\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4, #small to save memory\n",
    "    per_device_eval_batch_size=8, #small to save memory\n",
    "    learning_rate=2e-5, #standard for deberta; maybe try 6e-6\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_steps=50,\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    preds = p.predictions.argmax(-1)\n",
    "    return {'accuracy': accuracy_score(p.label_ids, preds)}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mps.empty_cache()  # Clears unused GPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disable upper limit for memory\n",
    "os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.0\"\n",
    "\n",
    "# Allows up to 100% of available memory\n",
    "torch.mps.set_per_process_memory_fraction(1.0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2625' max='2625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2625/2625 32:26, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.939800</td>\n",
       "      <td>0.961467</td>\n",
       "      <td>0.682667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.740400</td>\n",
       "      <td>0.926966</td>\n",
       "      <td>0.731333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.550900</td>\n",
       "      <td>1.017781</td>\n",
       "      <td>0.749333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.7313\n"
     ]
    }
   ],
   "source": [
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = trainer.evaluate()\n",
    "print(f\"Test Accuracy: {results['eval_accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = trainer.predict(test_dataset)\n",
    "predictions = np.argmax(output.predictions, axis=1)\n",
    "y_true = output.label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       precision    recall  f1-score   support\n",
      "\n",
      "           ad_hominem       0.71      0.60      0.65       159\n",
      "  appeal_to_authority       0.69      0.32      0.44        97\n",
      "    appeal_to_emotion       0.73      0.66      0.69       227\n",
      "        false_dilemma       0.82      0.56      0.67       133\n",
      "faulty_generalization       0.58      0.63      0.60       202\n",
      "                 none       0.77      0.91      0.83       682\n",
      "\n",
      "             accuracy                           0.73      1500\n",
      "            macro avg       0.72      0.61      0.65      1500\n",
      "         weighted avg       0.73      0.73      0.72      1500\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 96   6  25   2  11  19]\n",
      " [  6  31   9   0  14  37]\n",
      " [ 11   2 150   5  24  35]\n",
      " [  3   0   5  75   9  41]\n",
      " [  6   3  12   2 127  52]\n",
      " [ 14   3   5   7  35 618]]\n"
     ]
    }
   ],
   "source": [
    "# Generate classification report\n",
    "print(classification_report(y_true, predictions, target_names=le.classes_))\n",
    "\n",
    "# Generate confusion matrix\n",
    "cm = confusion_matrix(y_true, predictions)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I saved the model here as a SK model, although it is a Pytorch model. Keep that in mind!\n",
    "\n",
    "Save model:\n",
    "import mlflow.pytorch\n",
    "mlflow.pytorch.save_model(model, \"deberta_model\")\n",
    "\n",
    "Load model (correct way):\n",
    "model = mlflow.pytorch.load_model(\"deberta_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../models/LLM_deberta_v3_small\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(sk_model=model, path=path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow.sklearn\n",
    "model = mlflow.sklearn.load_model(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions based on reloaded model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the code works, although I loaded it as a Sklearn model, because I manually converted the logict to probabilites with torch.softmax. \n",
    "\n",
    "mlflow.sklearn.load_model() accidentally worked because MLflow can sometimes load PyTorch models as generic Python objects, but this isn't reliable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # batch can be changed both codes, now used the upper one\n",
    "# # upper one makes it more generalized for dynamic inputs, as the lower one only handles input_ids and attention_mask\n",
    "# batch = {\n",
    "#     key: val[i:i+batch_size].to(device) \n",
    "#     for key, val in encodings.items()\n",
    "# }\n",
    "\n",
    "# batch = {\n",
    "#                 \"input_ids\": encodings[\"input_ids\"][i:i+batch_size].to(device),\n",
    "#                 \"attention_mask\": encodings[\"attention_mask\"][i:i+batch_size].to(device)\n",
    "#             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function for prediction\n",
    "\n",
    "def predict(model, encodings, batch_size=8):\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Use GPU\n",
    "    device = torch.device(\"mps\")\n",
    "    model.to(device)\n",
    "    \n",
    "    # Perform inference\n",
    "    probabilities = []\n",
    "    for i in range(0, len(encodings[\"input_ids\"]), batch_size):\n",
    "        with torch.no_grad():\n",
    "            batch = {\n",
    "                key: val[i:i+batch_size].to(device) \n",
    "                for key, val in encodings.items()\n",
    "            }\n",
    "            outputs = model(**batch)\n",
    "            probs = torch.softmax(outputs.logits, dim=-1).cpu().numpy()\n",
    "            probabilities.extend(probs)\n",
    "            \n",
    "        # Clear GPU memory after each batch\n",
    "        torch.mps.empty_cache()\n",
    "    \n",
    "    return np.array(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#needed to reduce the batch size, otherwise I had an error\n",
    "# Get predictions for test data\n",
    "base_probs = predict(model, test_encodings, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get highest probability indices\n",
    "predicted_labels = np.argmax(base_probs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#probabilites\n",
    "predicted_label_probs = base_probs[np.arange(len(predicted_labels)), predicted_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       precision    recall  f1-score   support\n",
      "\n",
      "           ad_hominem       0.71      0.60      0.65       159\n",
      "  appeal_to_authority       0.69      0.32      0.44        97\n",
      "    appeal_to_emotion       0.73      0.66      0.69       227\n",
      "        false_dilemma       0.82      0.56      0.67       133\n",
      "faulty_generalization       0.58      0.63      0.60       202\n",
      "                 none       0.77      0.91      0.83       682\n",
      "\n",
      "             accuracy                           0.73      1500\n",
      "            macro avg       0.72      0.61      0.65      1500\n",
      "         weighted avg       0.73      0.73      0.72      1500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(y_test, predicted_labels, target_names=le.classes_)\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
