{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f94bd545",
   "metadata": {},
   "source": [
    "# q3fer/distilbert-base-fallacy-classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cda090",
   "metadata": {},
   "source": [
    "### Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27790c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alicepope/Capstone project/Capstone_project/backend/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, TextClassificationPipeline \n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, classification_report, confusion_matrix\n",
    "import mlflow\n",
    "from mlflow.transformers import log_model\n",
    "import logging \n",
    "from logging import getLogger\n",
    "from transformers import AutoConfig\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TextClassificationPipeline\n",
    "import torch\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "#  import pickle\n",
    "import warnings # why? \n",
    "from mlflow.sklearn import save_model \n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # This tells Hugging Face: “Don’t use parallel tokenization — avoid possible deadlocks.”\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import config "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d73bdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from basic_functions import (\n",
    "    get_eval_metrics,\n",
    "    createTrainer, \n",
    "    get_encode_tokenize_data\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee9709ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"q3fer/distilbert-base-fallacy-classification\" # pulls the fallacy trained model\n",
    "TRACKING_URI = config.TRACKING_URI\n",
    "EXPERIMENT_NAME = config.EXPERIMENT_NAME\n",
    "configuration = AutoConfig.from_pretrained(MODEL_NAME)\n",
    "\n",
    "logging.basicConfig(format=\"%(asctime)s: %(message)s\") # Configure logging format to show timestamp before every message\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO) # Only show logs that are INFO or more important (e.g., WARNING, ERROR) — but ignore DEBUG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aeba3e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Label ID to Label Mapping:\n",
      "{0: 'ad hominem', 1: 'ad populum', 2: 'appeal to emotion', 3: 'circular reasoning', 4: 'equivocation', 5: 'fallacy of credibility', 6: 'fallacy of extension', 7: 'fallacy of logic', 8: 'fallacy of relevance', 9: 'false causality', 10: 'false dilemma', 11: 'faulty generalization', 12: 'intentional', 13: 'miscellaneous'}\n",
      "\n",
      "Reverse Mapping (Label to ID):\n",
      "{'ad hominem': 0, 'ad populum': 1, 'appeal to emotion': 2, 'circular reasoning': 3, 'equivocation': 4, 'fallacy of credibility': 5, 'fallacy of extension': 6, 'fallacy of logic': 7, 'fallacy of relevance': 8, 'false causality': 9, 'false dilemma': 10, 'faulty generalization': 11, 'intentional': 12, 'miscellaneous': 13}\n"
     ]
    }
   ],
   "source": [
    "# Load model config to inspect label mappings\n",
    "\n",
    "print(\"Model Label ID to Label Mapping:\")\n",
    "print(configuration.id2label)\n",
    "\n",
    "print(\"\\nReverse Mapping (Label to ID):\")\n",
    "print(configuration.label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d142a532",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"../data/data_dropped_duplicates_small.csv\"\n",
    "MODEL_PATH = \"q3fer/distilbert-base-fallacy-classification\"\n",
    "MODEL_TRAINING_PATH =\"q3fer/distilbert-base-fallacy-classification\"\n",
    "OUTPUT_DIR = \"../models/q3fer/distilbert-base-fallacy-classification/trainer_output\"\n",
    "SAVE_PATH = \"../models/q3fer/distilbert-base-fallacy-classification/pytorch_model\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f31bd6",
   "metadata": {},
   "source": [
    "### Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb1d16ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:basic_functions:../data/data_dropped_duplicates_small.csv\n",
      "INFO:basic_functions:q3fer/distilbert-base-fallacy-classification\n",
      "INFO:basic_functions:Loading data...\n",
      "INFO:basic_functions:0                  ad_hominem\n",
      "1                        none\n",
      "2           appeal_to_emotion\n",
      "3           appeal_to_emotion\n",
      "4       faulty_generalization\n",
      "                ...          \n",
      "4995        appeal_to_emotion\n",
      "4996    faulty_generalization\n",
      "4997               ad_hominem\n",
      "4998                     none\n",
      "4999                     none\n",
      "Name: logical_fallacies, Length: 5000, dtype: object\n",
      "INFO:basic_functions:Train test split, test-size 0.3\n",
      "INFO:root:encode the label column\n",
      "INFO:root:tokenize\n",
      "INFO:basic_functions:create tokenizer & load model\n",
      "INFO:basic_functions:create tokenizer & load model\n",
      "INFO:root:create TextDatasets (train & test)\n"
     ]
    }
   ],
   "source": [
    "train_dataset, test_dataset, y_train, le = get_encode_tokenize_data(DATA_PATH, MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0740229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define new labels\n",
    "id2label = {\n",
    "    0: \"ad_hominem\",\n",
    "    1: \"appeal_to_authority\",\n",
    "    2: \"appeal_to_emotion\",\n",
    "    3: \"false_dilemma\",\n",
    "    4: \"faulty_generalization\",\n",
    "    5: \"none\"\n",
    "}\n",
    "\n",
    "label2id = {v: k for k, v in id2label.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd46821",
   "metadata": {},
   "source": [
    "### Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90c2c16d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = len(np.unique(y_train))\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "981718bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at q3fer/distilbert-base-fallacy-classification and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([14, 768]) in the checkpoint and torch.Size([6, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([14]) in the checkpoint and torch.Size([6]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME, \n",
    "    num_labels=num_classes,\n",
    "    ignore_mismatched_sizes= True\n",
    "    )\n",
    "model.config.id2label = id2label\n",
    "model.config.label2id = label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52e23fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 768])\n"
     ]
    }
   ],
   "source": [
    "final_layer = model.classifier\n",
    "# Check the size of the weights\n",
    "print(final_layer.weight.shape)  # Will be [num_labels, hidden_size] (num_labels x hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b03955f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBertConfig {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"ad_hominem\",\n",
      "    \"1\": \"appeal_to_authority\",\n",
      "    \"2\": \"appeal_to_emotion\",\n",
      "    \"3\": \"false_dilemma\",\n",
      "    \"4\": \"faulty_generalization\",\n",
      "    \"5\": \"none\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"ad_hominem\": 0,\n",
      "    \"appeal_to_authority\": 1,\n",
      "    \"appeal_to_emotion\": 2,\n",
      "    \"false_dilemma\": 3,\n",
      "    \"faulty_generalization\": 4,\n",
      "    \"none\": 5\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.50.3\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To see a summary of the model\n",
    "print(model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7558dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "      \"learning_rate\": 5e-5,\n",
    "      \"weight_decay\": 0.01,\n",
    "      \"num_train_epochs\": 3,\n",
    "      \"evaluation_strategy\": \"epoch\",\n",
    "      \"class_weight\":True,\n",
    "  }\n",
    "\n",
    "\n",
    "# setting the MLFlow connection and experiment\n",
    "mlflow.set_tracking_uri(TRACKING_URI)\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "\n",
    "mlflow.start_run()\n",
    "run = mlflow.active_run()\n",
    "print(\"Active run_id: {}\".format(run.info.run_id))\n",
    "\n",
    "mlflow.set_tag(\"model_name\", MODEL_NAME)\n",
    "mlflow.log_params(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ca0d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = createTrainer(\n",
    "    model= model, \n",
    "    train_dataset = train_dataset,\n",
    "    test_dataset = test_dataset,\n",
    "    output_dir= OUTPUT_DIR, \n",
    "    y_train=y_train, \n",
    "    class_weight=True, \n",
    "    epochs=3, \n",
    "    learning_rate=5e-5, \n",
    "    weight_decay = 0.01, \n",
    "    train_batch_size=4, \n",
    "    eval_batch_size=8 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4271cb9a",
   "metadata": {},
   "source": [
    "### Execute Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704a323d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mps.empty_cache()  # Clears unused GPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6c499e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# disable upper limit for memory\n",
    "os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.0\"\n",
    "\n",
    "# Allows up to 100% of available memory\n",
    "torch.mps.set_per_process_memory_fraction(1.0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137c00ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info('training is running')\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edda4f01",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e808b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add prediction and reporting here\n",
    "output = trainer.predict(dataset[\"test\"])\n",
    "predictions = np.argmax(output.predictions, axis=1)\n",
    "y_true = output.label_ids\n",
    "\n",
    "# Classification report\n",
    "label_names = le.classes_\n",
    "print(classification_report(y_true, predictions, target_names=label_names))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_true, predictions)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# plotting\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=label_names, yticklabels=label_names)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291d4317",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../models/distilbert-base-fallacy-classification\"\n",
    "save_model(sk_model=model, path=path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ff82fd",
   "metadata": {},
   "source": [
    "### Checking the q3fer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a61895a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference with q3fer model on sample texts\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "pipeline = TextClassificationPipeline(model=model, tokenizer=tokenizer)\n",
    "\n",
    "examples = [\n",
    "    \"You have no idea what you're talking about; you've only lived here for six months.\", # ad_hominem\n",
    "    \"I read a book by a nutritionist who says all carbs are bad.\", # appeal_to_authority\n",
    "    \"Can I have the last piece of cake? You know how much I love it, and it's been a tough day for me.\", # appeal_to_emotion\n",
    "    \"If we don't order pizza for dinner, we'll have to eat the week-old spaghetti in the fridge.\", # false_dilemma\n",
    "    \"I was in Greece for two week. Greeks are amazing people!\", # faulty_generalization\n",
    "    \"We should look into the science that supports this idea.\" # none\n",
    "]\n",
    "\n",
    "predictions = pipeline(examples)\n",
    "\n",
    "for i, pred in enumerate(predictions):\n",
    "    print(f\"Text {i+1}: {examples[i]}\")\n",
    "    print(f\"Prediction: {pred['label']} (Score: {pred['score']:.2f})\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4803b7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = {\n",
    "    'ad hominem': 'ad_hominem',\n",
    "    'appeal to emotion': 'appeal_to_emotion',\n",
    "    'false dilemma': 'false_dilemma',\n",
    "    'faulty generalization': 'faulty_generalization',\n",
    "    'circular reasoning': 'other',\n",
    "    'appeal to authority': 'appeal_to_authority',  \n",
    "    'miscellaneous': 'other',  \n",
    "    'fallacy of logic': 'other',\n",
    "    'intentional': 'other',\n",
    "    'ad populum': 'other',\n",
    "    'equivocation': 'other',\n",
    "    'false causality': 'other',\n",
    "    'fallacy of relevance': 'other',\n",
    "    'fallacy of credibility': 'other',\n",
    "    'fallacy of extension': 'other'\n",
    "}\n",
    "\n",
    "#  the model's label doesn't include 'none'.Maybe we should/could test it out on our dataset with only the fallacies?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5c133d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing with the examples\n",
    "mapped_predictions = []\n",
    "\n",
    "for i, pred in enumerate(predictions):\n",
    "    original_label = pred['label']\n",
    "    mapped_label = label_mapping.get(original_label, 'unmapped')\n",
    "\n",
    "    print(f\"Text {i+1}: {examples[i]}\")\n",
    "    print(f\"Original Prediction: {original_label} (Score: {pred['score']:.2f})\")\n",
    "    print(f\"Mapped to: {mapped_label}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    mapped_predictions.append(mapped_label)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
