{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeBERTa-v3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/katharinabaumgartner/Documents/NeueFische/scripts/34_capstone/Capstone_project/backend/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import mlflow\n",
    "from mlflow.transformers import log_model\n",
    "import logging \n",
    "from mlflow.sklearn import save_model\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from mlflow.models.signature import infer_signature\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from scipy.special import softmax\n",
    "import numpy as np\n",
    "\n",
    "from torch import nn\n",
    "import mlflow.pytorch\n",
    "\n",
    "import sentencepiece\n",
    "import os\n",
    "\n",
    "# os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # This tells Hugging Face: “Don’t use parallel tokenization — avoid possible deadlocks.”\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "from transformers import TrainingArguments, Trainer, AutoModelForSequenceClassification, AutoModel, AutoTokenizer, AutoConfig\n",
    "\n",
    "import config "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLFlow setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"deberta_v3\" \n",
    "TRACKING_URI = open(\"../.mlflow_uri\").read().strip()\n",
    "EXPERIMENT_NAME = config.EXPERIMENT_NAME\n",
    "\n",
    "logging.basicConfig(format=\"%(asctime)s: %(message)s\") # Configure logging format to show timestamp before every message\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO) # Only show logs that are INFO or more important (e.g., WARNING, ERROR) — but ignore DEBUG."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"../data/data_small.csv\"\n",
    "MODEL_PATH = \"microsoft/deberta-v3-base\"\n",
    "MODEL_TRAINING_PATH =\"microsoft/deberta-v3-small\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info('create tokenizer & load model')\n",
    "# tokenization after train test split to prevent data leakage\n",
    "#added use_fast=False to prevent tokenization error (might happen when using fast tokenization)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(texts):\n",
    "    return tokenizer(\n",
    "        texts,\n",
    "        padding=\"max_length\", #ensures that all tokenized sequences are padded to the same length, padding adds special tokens to shorter sequeces so they match the maximum length\n",
    "        truncation=True, #if sequence exceeds max, it will be trucated\n",
    "        max_length=512, #for most transformer models, 512 is a common limit for maximum length\n",
    "        return_tensors=\"pt\" #converts the output to pytorch tensors\n",
    "    )\n",
    "\n",
    "#object oriented programming (class is the object), with class you can do different things, such as calling functions\n",
    "class TextDataset(Dataset):  # Inherits from PyTorch's Dataset class\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.input_ids = encodings['input_ids']       # Token IDs from tokenizer\n",
    "        self.attention_mask = encodings['attention_mask']  # Mask for padding\n",
    "        self.labels = torch.tensor(labels)  # Convert labels to tensors\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.input_ids[idx],       # Token IDs for one sample\n",
    "            'attention_mask': self.attention_mask[idx],  # Mask for one sample\n",
    "            'labels': self.labels[idx]              # Label for one sample\n",
    "        }\n",
    "    def __len__(self):\n",
    "        return len(self.labels)  # Total number of samples\n",
    "\n",
    "def get_encode_tokenize_data(path, model_path):\n",
    "    logger.info(\"Loading data...\")\n",
    "    df = pd.read_csv(path)\n",
    "    y = df[\"logical_fallacies\"]\n",
    "    X = df[\"text\"]\n",
    "    logger.info(\"Train test split, test-size 0.3\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.30, random_state=42)\n",
    "\n",
    "    logging.info('encode the label column')\n",
    "    le = LabelEncoder()\n",
    "    y_train = le.fit_transform(y_train) \n",
    "    y_test = le.transform(y_test)\n",
    "\n",
    "    logging.info('tokenize')\n",
    "    train_encodings = tokenize(X_train.to_list())\n",
    "    test_encodings = tokenize(X_test.to_list())\n",
    "\n",
    "    logging.info('create TextDatasets (train & test)')\n",
    "    train_dataset = TextDataset(train_encodings, y_train)\n",
    "    test_dataset = TextDataset(test_encodings, y_test)\n",
    "\n",
    "    return train_dataset, test_dataset, y_train, le\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset, y_train, le = get_encode_tokenize_data(DATA_PATH, MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('../data/data_small.csv')\n",
    "\n",
    "# Y = df[\"logical_fallacies\"]\n",
    "# X = df[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     X, Y, test_size=0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/76868251/how-to-load-deberta-v3-properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenization after train test split to prevent data leakage\n",
    "\n",
    "#added use_fast=False to prevent tokenization error (might happen when using fast tokenization)\n",
    "# tokenizer = AutoTokenizer.from_pretrained('microsoft/deberta-v3-base', use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tokenize(texts):\n",
    "#     return tokenizer(\n",
    "#         texts,\n",
    "#         padding=\"max_length\", #ensures that all tokenized sequences are padded to the same length, padding adds special tokens to shorter sequeces so they match the maximum length\n",
    "#         truncation=True, #if sequence exceeds max, it will be trucated\n",
    "#         max_length=512, #for most transformer models, 512 is a common limit for maximum length\n",
    "#         return_tensors=\"pt\" #converts the output to pytorch tensors\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_encodings = tokenize(X_train.to_list())\n",
    "# test_encodings = tokenize(X_test.to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert string labels to integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# le = LabelEncoder()\n",
    "# y_train = le.fit_transform(y_train) \n",
    "# y_test = le.transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Preparation for usage in model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "needed to create a PyTorch Dataset object that:\n",
    "- organizes tokenized text\n",
    "- pairs them with corresponding labels\n",
    "- structures everything for batch processing during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #object oriented programming (class is the object), with class you can do different things, such as calling functions\n",
    "\n",
    "# class TextDataset(Dataset):  # Inherits from PyTorch's Dataset class\n",
    "#     def __init__(self, encodings, labels):\n",
    "#         self.input_ids = encodings['input_ids']       # Token IDs from tokenizer\n",
    "#         self.attention_mask = encodings['attention_mask']  # Mask for padding\n",
    "#         self.labels = torch.tensor(labels)  # Convert labels to tensors\n",
    "#     def __getitem__(self, idx):\n",
    "#         return {\n",
    "#             'input_ids': self.input_ids[idx],       # Token IDs for one sample\n",
    "#             'attention_mask': self.attention_mask[idx],  # Mask for one sample\n",
    "#             'labels': self.labels[idx]              # Label for one sample\n",
    "#         }\n",
    "#     def __len__(self):\n",
    "#         return len(self.labels)  # Total number of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = TextDataset(train_encodings, y_train)\n",
    "# test_dataset = TextDataset(test_encodings, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero Shot Inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # disable upper limit for memory\n",
    "# os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.0\"\n",
    "\n",
    "# # Allows up to 100% of available memory\n",
    "# torch.mps.set_per_process_memory_fraction(1.0)  \n",
    "\n",
    "# torch.mps.empty_cache()  # Clears unused GPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load fresh copy of base model (not train on our data)\n",
    "# num_classes = len(df[\"logical_fallacies\"].unique())\n",
    "# base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "#     \"microsoft/deberta-v3-small\",\n",
    "#     num_labels=num_classes,\n",
    "#     problem_type=\"single_label_classification\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predict(model, encodings, batch_size=8):\n",
    "#     # Set the model to evaluation mode\n",
    "#     model.eval()\n",
    "    \n",
    "#     # Use GPU\n",
    "#     device = torch.device(\"mps\")\n",
    "#     model.to(device)\n",
    "    \n",
    "#     # Perform inference\n",
    "#     probabilities = []\n",
    "#     for i in range(0, len(encodings[\"input_ids\"]), batch_size):\n",
    "#         with torch.no_grad():\n",
    "#             batch = {\n",
    "#                 \"input_ids\": encodings[\"input_ids\"][i:i+batch_size].to(device),\n",
    "#                 \"attention_mask\": encodings[\"attention_mask\"][i:i+batch_size].to(device)\n",
    "#             }\n",
    "#             outputs = model(**batch)\n",
    "#             probs = torch.softmax(outputs.logits, dim=-1).cpu().numpy()\n",
    "#             probabilities.extend(probs)\n",
    "            \n",
    "#         # Clear GPU memory after each batch\n",
    "#         torch.mps.empty_cache()\n",
    "    \n",
    "#     return np.array(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get predictions for test data\n",
    "# base_probs = predict(base_model, test_encodings, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get highest probability indices\n",
    "# predicted_indices = np.argmax(base_probs, axis=1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import classification_report\n",
    "\n",
    "# # Generate classification report\n",
    "# report = classification_report(y_test, predicted_indices, target_names=le.classes_)\n",
    "# print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: This deberta model is actually not designed for zero shot, there is one by MoritzLauer which can be used without requiring training on data. So training on data is actually necessary! The DeBERTa used here is meant for supervised learning. \n",
    "Another option is to use BART, facebook/bart-large-mnli model.\n",
    "\n",
    "**Zero-Shot Learning** </span> is a concept, that a model when trained on enough unlabeled data (unsupervised learning) is able to generalize/ recognize at inference time even though the model was not trained on the inference data. This can be used in NLP, Images etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I had to change configuration of accelerate, as it might still be configured to fp16 (mixed precision)(doesn't work on Apple M1 Pro):\n",
    "- type in bash accelerate confic\n",
    "- this machine\n",
    "- no distributed training\n",
    "- do you want to run your training on CPU only, say No, as MAC Apple M1 Pro has GPU\n",
    "- do you wish to optimize script with torch dynamo: say \"No\" if using an Apple M1 Pro with MPS backend\n",
    "- do you want to use mixed prexision: NO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(np.unique(y_train))\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_TRAINING_PATH,\n",
    "    num_labels=num_classes,\n",
    "    problem_type=\"single_label_classification\"\n",
    ")\n",
    "\n",
    "model.gradient_checkpointing_enable()  # force model to use gradient checkpointing to save memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = \"../models/LLM_deberta_v3_small_class_imbalance/trainer_output\"\n",
    "SAVE_PATH = \"../models/LLM_deberta_v3_small_class_imbalance/pytorch_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedLossTrainer(Trainer):\n",
    "    def __init__(self, class_weights=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.class_weights = class_weights\n",
    "        if self.class_weights is not None:\n",
    "            # Move weights to device after model initialization\n",
    "            self._move_weights_to_device()\n",
    "    \n",
    "    def _move_weights_to_device(self):\n",
    "        self.class_weights = self.class_weights.to(self.model.device)\n",
    "\n",
    "    def compute_loss(\n",
    "        self, \n",
    "        model, \n",
    "        inputs, \n",
    "        return_outputs=False, \n",
    "        num_items_in_batch=None  # Add this parameter\n",
    "    ):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        loss_fct = nn.CrossEntropyLoss(weight=self.class_weights)\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), \n",
    "                       labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "        \n",
    "\n",
    "def createTrainer(\n",
    "    output_dir,\n",
    "    y_train, \n",
    "    class_weight=False, \n",
    "    epochs=3, \n",
    "    learning_rate=5e-5, \n",
    "    weight_decay = 0, \n",
    "    train_batch_size = 4, \n",
    "    eval_batch_size=8\n",
    "    ):\n",
    "    logging.info('defining training arguments')\n",
    "    training_args = TrainingArguments(\n",
    "            output_dir=output_dir, # to sve results\n",
    "            num_train_epochs=epochs,\n",
    "            per_device_train_batch_size=train_batch_size, #small to save memory\n",
    "            per_device_eval_batch_size=eval_batch_size, #small to save memory\n",
    "            learning_rate=learning_rate, #standard for deberta; maybe try 6e-6\n",
    "            weight_decay=weight_decay,\n",
    "            eval_strategy=\"epoch\",\n",
    "            logging_steps=50,\n",
    "            save_strategy=\"epoch\",\n",
    "            load_best_model_at_end=True\n",
    "        )\n",
    "\n",
    "    computed_class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
    "    # Convert class weights to tensor\n",
    "    computed_class_weights_tensor = torch.tensor(computed_class_weights, dtype=torch.float32)\n",
    "\n",
    "    def compute_metrics(p):\n",
    "        preds = p.predictions.argmax(-1)\n",
    "        return {'accuracy': accuracy_score(p.label_ids, preds)}\n",
    "\n",
    "    if class_weight==False:\n",
    "        logging.info('get normal trainer')\n",
    "        return Trainer(\n",
    "                model=model,\n",
    "                args=training_args,\n",
    "                train_dataset=train_dataset,\n",
    "                eval_dataset=test_dataset,\n",
    "                compute_metrics=compute_metrics\n",
    "            )\n",
    "    else:\n",
    "        logging.info('get weighted loss trainer')\n",
    "        return WeightedLossTrainer(\n",
    "                model=model,\n",
    "                args=training_args,\n",
    "                train_dataset=train_dataset,\n",
    "                eval_dataset=test_dataset,\n",
    "                compute_metrics=compute_metrics,\n",
    "                class_weights=computed_class_weights_tensor\n",
    "            )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch = 3\n",
    "# learning_rate=2e-5 #standard for deberta; maybe try 6e-6\n",
    "# weight_decay=0.01\n",
    "# per_device_train_batch_size=4 #small to save memory\n",
    "# per_device_eval_batch_size=8 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = createTrainer(class_weight=True, output_dir= OUTPUT_DIR, y_train=y_train, epochs=1, learning_rate=2e-5, weight_decay = 0.01, train_batch_size=4, eval_batch_size=8 )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class-balanced trainer\n",
    "# class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert class weights to tensor\n",
    "# class_weights = torch.tensor(class_weights, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# class WeightedLossTrainer(Trainer):\n",
    "#     def __init__(self, class_weights=None, **kwargs):\n",
    "#         super().__init__(**kwargs)\n",
    "#         self.class_weights = class_weights\n",
    "#         if self.class_weights is not None:\n",
    "#             # Move weights to device after model initialization\n",
    "#             self._move_weights_to_device()\n",
    "    \n",
    "#     def _move_weights_to_device(self):\n",
    "#         self.class_weights = self.class_weights.to(self.model.device)\n",
    "\n",
    "#     def compute_loss(\n",
    "#         self, \n",
    "#         model, \n",
    "#         inputs, \n",
    "#         return_outputs=False, \n",
    "#         num_items_in_batch=None  # Add this parameter\n",
    "#     ):\n",
    "#         labels = inputs.pop(\"labels\")\n",
    "#         outputs = model(**inputs)\n",
    "#         logits = outputs.logits\n",
    "#         loss_fct = nn.CrossEntropyLoss(weight=self.class_weights)\n",
    "#         loss = loss_fct(logits.view(-1, self.model.config.num_labels), \n",
    "#                        labels.view(-1))\n",
    "#         return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Confguration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer with class weights\n",
    "\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir='../models/LLM_deberta_v3_small_class_imbalance/trainer_output', # to sve results\n",
    "#     num_train_epochs=3,\n",
    "#     per_device_train_batch_size=4, #small to save memory\n",
    "#     per_device_eval_batch_size=8, #small to save memory\n",
    "#     learning_rate=2e-5, #standard for deberta; maybe try 6e-6\n",
    "#     weight_decay=0.01,\n",
    "#     evaluation_strategy=\"epoch\",\n",
    "#     logging_steps=50,\n",
    "#     save_strategy=\"epoch\",\n",
    "#     load_best_model_at_end=True\n",
    "# )\n",
    "\n",
    "# def compute_metrics(p):\n",
    "#     preds = p.predictions.argmax(-1)\n",
    "#     return {'accuracy': accuracy_score(p.label_ids, preds)}\n",
    "\n",
    "# trainer = WeightedLossTrainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=train_dataset,\n",
    "#     eval_dataset=test_dataset,\n",
    "#     compute_metrics=compute_metrics,\n",
    "#     class_weights=class_weights\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # trainer without class weights\n",
    "\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir='../models/LLM_deberta_v3_small_class_imbalance/trainer_output', # to sve results\n",
    "#     num_train_epochs=3,\n",
    "#     per_device_train_batch_size=4, #small to save memory\n",
    "#     per_device_eval_batch_size=8, #small to save memory\n",
    "#     learning_rate=2e-5, #standard for deberta; maybe try 6e-6\n",
    "#     weight_decay=0.01,\n",
    "#     evaluation_strategy=\"epoch\",\n",
    "#     logging_steps=50,\n",
    "#     save_strategy=\"epoch\",\n",
    "#     load_best_model_at_end=True\n",
    "# )\n",
    "\n",
    "# def compute_metrics(p):\n",
    "#     preds = p.predictions.argmax(-1)\n",
    "#     return {'accuracy': accuracy_score(p.label_ids, preds)}\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=train_dataset,\n",
    "#     eval_dataset=test_dataset,\n",
    "#     compute_metrics=compute_metrics\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mps.empty_cache()  # Clears unused GPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disable upper limit for memory\n",
    "os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.0\"\n",
    "\n",
    "# Allows up to 100% of available memory\n",
    "torch.mps.set_per_process_memory_fraction(1.0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info('training is running')\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = trainer.evaluate()\n",
    "# print(f\"Test Accuracy: {results['eval_accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info('predict on test_dataset')\n",
    "output = trainer.predict(test_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_eval_metrics(output, le):\n",
    "    logger.info('get evaluation metrics')\n",
    "\n",
    "    y_pred = np.argmax(output.predictions, axis=1)\n",
    "    y_true = output.label_ids\n",
    "    logits = output.predictions\n",
    "    proba = softmax(logits, axis=1)\n",
    "\n",
    "    logger.info('classification_report')\n",
    "    print(classification_report(y_true, y_pred, target_names=le.classes_))\n",
    "\n",
    "    logger.info('confusion_matrix')\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "\n",
    "    logger.info('brier score')\n",
    "    # 1. One-hot encode the true labels (y_test)\n",
    "    lb = LabelBinarizer()\n",
    "    y_true_onehot = lb.fit_transform(y_true)  # Shape: (n_samples, n_classes)\n",
    "\n",
    "    # 2. Compute Brier score for multiclass\n",
    "    brier_score = np.mean(np.sum((proba - y_true_onehot) ** 2, axis=1))\n",
    "    print(\"Multiclass Brier score:\", brier_score)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_eval_metrics(output, le)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate classification report\n",
    "# print(classification_report(y_true, predictions, target_names=le.classes_))\n",
    "\n",
    "# # Generate confusion matrix\n",
    "# cm = confusion_matrix(y_true, predictions)\n",
    "# print(\"Confusion Matrix:\")\n",
    "# print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I saved the model here as a SK model, although it is a Pytorch model. Keep that in mind!\n",
    "\n",
    "Save model:\n",
    "import mlflow.pytorch\n",
    "mlflow.pytorch.save_model(model, \"deberta_model\")\n",
    "\n",
    "Load model (correct way):\n",
    "model = mlflow.pytorch.load_model(\"deberta_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #save with sklearn\n",
    "# path_sk = \"../models/LLM_deberta_v3_small_class_imbalance/sk_learn_model\"\n",
    "# save_model(sk_model=model, path=path_sk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save with pytorch\n",
    "\n",
    "mlflow.pytorch.save_model(model, path=SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow.sklearn\n",
    "path_sk = \"../models/LLM_deberta_v3_small_class_imbalance/sk_learn_model\"\n",
    "model = mlflow.sklearn.load_model(path_sk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow.pytorch\n",
    "path_pt = \"../models/LLM_deberta_v3_small_class_imbalance/pytorch_model\"\n",
    "model = mlflow.pytorch.load_model(path_pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions based on reloaded model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the code works, although I loaded it as a Sklearn model, because I manually converted the logits to probabilites with torch.softmax. \n",
    "\n",
    "mlflow.sklearn.load_model() accidentally worked because MLflow can sometimes load PyTorch models as generic Python objects, but this isn't reliable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # batch can be changed both codes, now used the upper one\n",
    "# # upper one makes it more generalized for dynamic inputs, as the lower one only handles input_ids and attention_mask\n",
    "# batch = {\n",
    "#     key: val[i:i+batch_size].to(device) \n",
    "#     for key, val in encodings.items()\n",
    "# }\n",
    "\n",
    "# batch = {\n",
    "#                 \"input_ids\": encodings[\"input_ids\"][i:i+batch_size].to(device),\n",
    "#                 \"attention_mask\": encodings[\"attention_mask\"][i:i+batch_size].to(device)\n",
    "#             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function for prediction\n",
    "\n",
    "def predict(model, encodings, batch_size=8):\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Use GPU\n",
    "    device = torch.device(\"mps\")\n",
    "    model.to(device)\n",
    "    \n",
    "    # Perform inference\n",
    "    probabilities = []\n",
    "    for i in range(0, len(encodings[\"input_ids\"]), batch_size):\n",
    "        with torch.no_grad():\n",
    "            batch = {\n",
    "                key: val[i:i+batch_size].to(device) \n",
    "                for key, val in encodings.items()\n",
    "            }\n",
    "            outputs = model(**batch)\n",
    "            probs = torch.softmax(outputs.logits, dim=-1).cpu().numpy()\n",
    "            probabilities.extend(probs)\n",
    "            \n",
    "        # Clear GPU memory after each batch\n",
    "        torch.mps.empty_cache()\n",
    "    \n",
    "    return np.array(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#needed to reduce the batch size, otherwise I had an error\n",
    "# Get predictions for test data\n",
    "base_probs = predict(model, test_encodings, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get highest probability indices\n",
    "predicted_labels = np.argmax(base_probs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get second highest probability indices\n",
    "second_predicted_labels = np.argsort(base_probs, axis=1)[:, -2]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#probabilites of first predicted\n",
    "predicted_label_probs = base_probs[np.arange(len(predicted_labels)), predicted_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#probabilites of second predicted\n",
    "second_predicted_label_probs = np.sort(base_probs, axis=1)[:, -2]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for backend \n",
    "result = {\n",
    "    \"predicted_labels\": predicted_labels,\n",
    "    \"predicted_label_probs\": predicted_label_probs,\n",
    "    \"second_predicted_labels\": second_predicted_labels,\n",
    "    \"second_predicted_label_probs\": second_predicted_label_probs\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(y_test, predicted_labels, target_names=le.classes_)\n",
    "print(report)\n",
    "\n",
    "# Generate confusion matrix\n",
    "cm = confusion_matrix(y_test, predicted_labels)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "# # 1. One-hot encode the true labels (y_test)\n",
    "# lb = LabelBinarizer()\n",
    "# y_true_onehot = lb.fit_transform(y_test)  # Shape: (n_samples, n_classes)\n",
    "\n",
    "# # 2. Compute Brier score for multiclass\n",
    "# brier_score = np.mean(np.sum((base_probs - y_true_onehot) ** 2, axis=1))\n",
    "# print(\"Multiclass Brier score:\", brier_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
