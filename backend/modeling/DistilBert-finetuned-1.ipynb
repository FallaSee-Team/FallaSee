{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DistilBert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import mlflow\n",
    "from mlflow.transformers import log_model\n",
    "import logging \n",
    "from mlflow.sklearn import save_model\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from mlflow.models.signature import infer_signature\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from scipy.special import softmax\n",
    "import numpy as np\n",
    "\n",
    "from torch import nn\n",
    "import mlflow.pytorch\n",
    "\n",
    "import sentencepiece\n",
    "import os\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # This tells Hugging Face: “Don’t use parallel tokenization — avoid possible deadlocks.”\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "from transformers import TrainingArguments, Trainer, AutoModelForSequenceClassification, AutoModel, AutoTokenizer, AutoConfig\n",
    "\n",
    "import config "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from basic_functions import(\n",
    "    get_encode_tokenize_data,\n",
    "    createTrainer,\n",
    "    get_eval_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"distilbert-base-uncased\" # pulls the general-purpose DistilBERT model\n",
    "TRACKING_URI = open(\"../.mlflow_uri\").read().strip()\n",
    "EXPERIMENT_NAME = config.EXPERIMENT_NAME\n",
    "\n",
    "logging.basicConfig(format=\"%(asctime)s: %(message)s\") # Configure logging format to show timestamp before every message\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO) # Only show logs that are INFO or more important (e.g., WARNING, ERROR) — but ignore DEBUG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"../data/data_tiny.csv\"\n",
    "MODEL_PATH = \"distilbert-base-uncased\"\n",
    "MODEL_TRAINING_PATH =\"distilbert-base-uncased\"\n",
    "OUTPUT_DIR = \"../models/distilbert_finetuned_1/trainer_output\"\n",
    "SAVE_PATH = \"../models/distilbert_finetuned_1/pytorch_model\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset, y_train, le = get_encode_tokenize_data(DATA_PATH, MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(np.unique(y_train))\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_TRAINING_PATH,\n",
    "    num_labels=num_classes,\n",
    "    problem_type=\"single_label_classification\"\n",
    ")\n",
    "\n",
    "# model.gradient_checkpointing_enable() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "      \"learning_rate\": 3e-5,\n",
    "      \"weight_decay\": 0.01,\n",
    "      \"num_train_epochs\": 4,\n",
    "      \"evaluation_strategy\": \"epoch\",\n",
    "  }\n",
    "\n",
    "\n",
    "# setting the MLFlow connection and experiment\n",
    "mlflow.set_tracking_uri(TRACKING_URI)\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "\n",
    "mlflow.start_run()\n",
    "run = mlflow.active_run()\n",
    "print(\"Active run_id: {}\".format(run.info.run_id))\n",
    "\n",
    "mlflow.set_tag(\"model_name\", MODEL_NAME)\n",
    "mlflow.log_params(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = createTrainer(\n",
    "    model= model, \n",
    "    train_dataset = train_dataset,\n",
    "    test_dataset = test_dataset,\n",
    "    output_dir= OUTPUT_DIR, \n",
    "    y_train=y_train, \n",
    "    class_weight=True, \n",
    "    epochs=1, \n",
    "    learning_rate=2e-5, \n",
    "    weight_decay = 0.01, \n",
    "    train_batch_size=4, \n",
    "    eval_batch_size=8 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mps.empty_cache()  # Clears unused GPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disable upper limit for memory\n",
    "os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.0\"\n",
    "\n",
    "# Allows up to 100% of available memory\n",
    "torch.mps.set_per_process_memory_fraction(1.0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info('training is running')\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_metrics(cr, brier, split):\n",
    "    mlflow.log_metric(f\"{split}_brier\", brier)\n",
    "\n",
    "    for key, value in cr.items():\n",
    "        if (key == \"accuracy\"):\n",
    "                # print(f\"{split}_{key}\", round(value,2))\n",
    "                mlflow.log_metric(f\"{split}_{key}\", value)\n",
    "        else:\n",
    "            for metric in value:\n",
    "                mlflow.log_metric(f\"{split}_{key}_{metric}\", value.get(metric))\n",
    "                # print(f\"{split}_{key}_{metric}\", round(value.get(metric),2))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info('predict on train_dataset')\n",
    "train_output = trainer.predict(train_dataset)\n",
    "\n",
    "classification_report, brier= get_eval_metrics(train_output, le)\n",
    "log_metrics(classification_report, brier, \"train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info('predict on test_dataset')\n",
    "test_output = trainer.predict(test_dataset)\n",
    "\n",
    "classification_report, brier = get_eval_metrics(test_output, le)\n",
    "log_metrics(classification_report, brier, \"test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save with pytorch\n",
    "mlflow.pytorch.save_model(model, path=SAVE_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
