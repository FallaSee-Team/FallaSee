{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DistilBert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alicepope/Capstone project/Capstone_project/backend/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# from sklearn.model_selection import train_test_split\n",
    "import mlflow\n",
    "# from mlflow.transformers import log_model\n",
    "import logging \n",
    "from mlflow.sklearn import save_model\n",
    "\n",
    "# from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# from mlflow.models.signature import infer_signature\n",
    "# from sklearn.utils.class_weight import compute_class_weight\n",
    "# from sklearn.preprocessing import LabelBinarizer\n",
    "# from scipy.special import softmax\n",
    "import numpy as np\n",
    "\n",
    "# from torch import nn\n",
    "# import mlflow.pytorch\n",
    "\n",
    "# import sentencepiece\n",
    "import os\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # This tells Hugging Face: “Don’t use parallel tokenization — avoid possible deadlocks.”\n",
    "\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "from transformers import TrainingArguments, Trainer, AutoModelForSequenceClassification, AutoModel, AutoTokenizer, AutoConfig\n",
    "\n",
    "import config "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from basic_functions import(\n",
    "    get_encode_tokenize_data,\n",
    "    createTrainer,\n",
    "    get_eval_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"distilbert-base-uncased\" # pulls the general-purpose DistilBERT model\n",
    "TRACKING_URI = open(\"../.mlflow_uri\").read().strip()\n",
    "EXPERIMENT_NAME = config.EXPERIMENT_NAME\n",
    "\n",
    "logging.basicConfig(format=\"%(asctime)s: %(message)s\") # Configure logging format to show timestamp before every message\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO) # Only show logs that are INFO or more important (e.g., WARNING, ERROR) — but ignore DEBUG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"../data/data_dropped_duplicates_small.csv\"\n",
    "MODEL_PATH = \"distilbert-base-uncased\"\n",
    "MODEL_TRAINING_PATH =\"distilbert-base-uncased\"\n",
    "OUTPUT_DIR = \"../models/distilbert_finetuned_1/trainer_output\"\n",
    "SAVE_PATH = \"../models/distilbert_finetuned_1/pytorch_model\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:basic_functions:Loading data...\n",
      "INFO:basic_functions:Train test split, test-size 0.3\n",
      "INFO:root:encode the label column\n",
      "INFO:root:tokenize\n",
      "INFO:basic_functions:create tokenizer & load model\n",
      "INFO:basic_functions:create tokenizer & load model\n",
      "INFO:root:create TextDatasets (train & test)\n"
     ]
    }
   ],
   "source": [
    "train_dataset, test_dataset, y_train, le = get_encode_tokenize_data(DATA_PATH, MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = len(np.unique(y_train))\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_TRAINING_PATH,\n",
    "    num_labels=num_classes,\n",
    "    problem_type=\"single_label_classification\"\n",
    ")\n",
    "\n",
    "# model.gradient_checkpointing_enable() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Active run_id: c6af1fede7ef45b1aa5248d23633d585\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "      \"learning_rate\": 3e-5,\n",
    "      \"weight_decay\": 0.01,\n",
    "      \"num_train_epochs\": 4,\n",
    "      \"evaluation_strategy\": \"epoch\",\n",
    "      \"class_weight\":True,\n",
    "  }\n",
    "\n",
    "\n",
    "# setting the MLFlow connection and experiment\n",
    "mlflow.set_tracking_uri(TRACKING_URI)\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "\n",
    "mlflow.start_run()\n",
    "run = mlflow.active_run()\n",
    "print(\"Active run_id: {}\".format(run.info.run_id))\n",
    "\n",
    "mlflow.set_tag(\"model_name\", MODEL_NAME)\n",
    "mlflow.log_params(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:defining training arguments\n",
      "INFO:root:get weighted loss trainer\n"
     ]
    }
   ],
   "source": [
    "trainer = createTrainer(\n",
    "    model= model, \n",
    "    train_dataset = train_dataset,\n",
    "    test_dataset = test_dataset,\n",
    "    output_dir= OUTPUT_DIR, \n",
    "    y_train=y_train, \n",
    "    class_weight=True, \n",
    "    epochs=4, \n",
    "    learning_rate=3e-5, \n",
    "    weight_decay = 0.01, \n",
    "    train_batch_size=4, \n",
    "    eval_batch_size=8 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mps.empty_cache()  # Clears unused GPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disable upper limit for memory\n",
    "os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.0\"\n",
    "\n",
    "# Allows up to 100% of available memory\n",
    "torch.mps.set_per_process_memory_fraction(1.0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:training is running\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3501' max='3500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3500/3500 5:12:37, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.988100</td>\n",
       "      <td>1.056663</td>\n",
       "      <td>0.618000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.716600</td>\n",
       "      <td>1.176395</td>\n",
       "      <td>0.735333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.387200</td>\n",
       "      <td>1.548002</td>\n",
       "      <td>0.740667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.296400</td>\n",
       "      <td>1.736059</td>\n",
       "      <td>0.732000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='438' max='438' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [438/438 20:53]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/04/07 10:14:09 ERROR mlflow.utils.async_logging.async_logging_queue: Run Id c6af1fede7ef45b1aa5248d23633d585: Failed to log run data: Exception: API request to http://127.0.0.1:5001/api/2.0/mlflow/runs/log-batch failed with exception HTTPConnectionPool(host='127.0.0.1', port=5001): Max retries exceeded with url: /api/2.0/mlflow/runs/log-batch (Caused by ResponseError('too many 500 error responses'))\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m logger.info(\u001b[33m'\u001b[39m\u001b[33mtraining is running\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Capstone project/Capstone_project/backend/.venv/lib/python3.11/site-packages/transformers/trainer.py:2245\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2243\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2244\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2245\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2246\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2250\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Capstone project/Capstone_project/backend/.venv/lib/python3.11/site-packages/transformers/trainer.py:2675\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2672\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m is_sagemaker_mp_enabled():\n\u001b[32m   2673\u001b[39m         smp.barrier()\n\u001b[32m-> \u001b[39m\u001b[32m2675\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_best_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2677\u001b[39m \u001b[38;5;66;03m# add remaining tr_loss\u001b[39;00m\n\u001b[32m   2678\u001b[39m \u001b[38;5;28mself\u001b[39m._total_loss_scalar += tr_loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Capstone project/Capstone_project/backend/.venv/lib/python3.11/site-packages/transformers/trainer.py:3016\u001b[39m, in \u001b[36mTrainer._load_best_model\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   3007\u001b[39m         state_dict = torch.load(\n\u001b[32m   3008\u001b[39m             best_model_path,\n\u001b[32m   3009\u001b[39m             map_location=\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   3010\u001b[39m             **weights_only_kwarg,\n\u001b[32m   3011\u001b[39m         )\n\u001b[32m   3013\u001b[39m     \u001b[38;5;66;03m# If the model is on the GPU, it still works!\u001b[39;00m\n\u001b[32m   3014\u001b[39m     \u001b[38;5;66;03m# workaround for FSDP bug https://github.com/pytorch/pytorch/issues/82963\u001b[39;00m\n\u001b[32m   3015\u001b[39m     \u001b[38;5;66;03m# which takes *args instead of **kwargs\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3016\u001b[39m     load_result = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   3017\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_sagemaker_mp_enabled() \u001b[38;5;129;01mand\u001b[39;00m has_been_loaded:\n\u001b[32m   3018\u001b[39m     \u001b[38;5;28mself\u001b[39m._issue_warnings_after_load(load_result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Capstone project/Capstone_project/backend/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2139\u001b[39m, in \u001b[36mModule.load_state_dict\u001b[39m\u001b[34m(self, state_dict, strict, assign)\u001b[39m\n\u001b[32m   2132\u001b[39m         out = hook(module, incompatible_keys)\n\u001b[32m   2133\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, (\n\u001b[32m   2134\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mHooks registered with ``register_load_state_dict_post_hook`` are not\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2135\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mexpected to return new values, if incompatible_keys need to be modified,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2136\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mit should be done inplace.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2137\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m2139\u001b[39m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2140\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m load\n\u001b[32m   2142\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m strict:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Capstone project/Capstone_project/backend/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:2121\u001b[39m, in \u001b[36mModule.load_state_dict.<locals>.load\u001b[39m\u001b[34m(module, local_state_dict, prefix)\u001b[39m\n\u001b[32m   2119\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m assign:\n\u001b[32m   2120\u001b[39m     local_metadata[\u001b[33m'\u001b[39m\u001b[33massign_to_params_buffers\u001b[39m\u001b[33m'\u001b[39m] = assign\n\u001b[32m-> \u001b[39m\u001b[32m2121\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_load_from_state_dict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2122\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_state_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_metadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_msgs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2123\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, child \u001b[38;5;129;01min\u001b[39;00m module._modules.items():\n\u001b[32m   2124\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m child \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Capstone project/Capstone_project/backend/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1953\u001b[39m, in \u001b[36mModule._load_from_state_dict\u001b[39m\u001b[34m(self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs)\u001b[39m\n\u001b[32m   1949\u001b[39m     \u001b[38;5;28mself\u001b[39m._load_state_dict_post_hooks[handle.id] = hook\n\u001b[32m   1950\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle\n\u001b[32m-> \u001b[39m\u001b[32m1953\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_load_from_state_dict\u001b[39m(\u001b[38;5;28mself\u001b[39m, state_dict, prefix, local_metadata, strict,\n\u001b[32m   1954\u001b[39m                           missing_keys, unexpected_keys, error_msgs):\n\u001b[32m   1955\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Copy parameters and buffers from :attr:`state_dict` into only this module, but not its descendants.\u001b[39;00m\n\u001b[32m   1956\u001b[39m \n\u001b[32m   1957\u001b[39m \u001b[33;03m    This is called on every submodule\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1988\u001b[39m \u001b[33;03m            :meth:`~torch.nn.Module.load_state_dict`\u001b[39;00m\n\u001b[32m   1989\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   1990\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._load_state_dict_pre_hooks.values():\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "logger.info('training is running')\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_metrics(cr, brier, split):\n",
    "    mlflow.log_metric(f\"{split}_brier\", brier)\n",
    "\n",
    "    for key, value in cr.items():\n",
    "        if (key == \"accuracy\"):\n",
    "                # print(f\"{split}_{key}\", round(value,2))\n",
    "                mlflow.log_metric(f\"{split}_{key}\", value)\n",
    "        else:\n",
    "            for metric in value:\n",
    "                mlflow.log_metric(f\"{split}_{key}_{metric}\", value.get(metric))\n",
    "                # print(f\"{split}_{key}_{metric}\", round(value.get(metric),2))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:predict on test_dataset\n",
      "INFO:basic_functions:get evaluation metrics\n",
      "INFO:basic_functions:classification_report\n",
      "INFO:basic_functions:confusion_matrix\n",
      "INFO:basic_functions:brier score\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       precision    recall  f1-score   support\n",
      "\n",
      "           ad_hominem       0.69      0.61      0.65       142\n",
      "  appeal_to_authority       0.71      0.56      0.62        97\n",
      "    appeal_to_emotion       0.68      0.73      0.70       216\n",
      "        false_dilemma       0.73      0.69      0.71       137\n",
      "faulty_generalization       0.61      0.49      0.55       192\n",
      "                 none       0.78      0.85      0.82       716\n",
      "\n",
      "             accuracy                           0.73      1500\n",
      "            macro avg       0.70      0.66      0.67      1500\n",
      "         weighted avg       0.73      0.73      0.73      1500\n",
      "\n",
      "[[ 87   3  22   1   4  25]\n",
      " [  5  54   6   2   3  27]\n",
      " [ 16   3 157   3  12  25]\n",
      " [  3   1   1  95   4  33]\n",
      " [  7   8  20   2  95  60]\n",
      " [  9   7  25  27  38 610]]\n",
      "Multiclass Brier score: 0.47276299786606957\n"
     ]
    }
   ],
   "source": [
    "logger.info('predict on test_dataset')\n",
    "test_output = trainer.predict(test_dataset)\n",
    "\n",
    "classification_report, brier = get_eval_metrics(test_output, le)\n",
    "log_metrics(classification_report, brier, \"test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:predict on train_dataset\n",
      "INFO:basic_functions:get evaluation metrics\n",
      "INFO:basic_functions:classification_report\n",
      "INFO:basic_functions:confusion_matrix\n",
      "INFO:basic_functions:brier score\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       precision    recall  f1-score   support\n",
      "\n",
      "           ad_hominem       0.98      0.98      0.98       331\n",
      "  appeal_to_authority       0.99      0.99      0.99       227\n",
      "    appeal_to_emotion       0.98      0.97      0.98       504\n",
      "        false_dilemma       0.98      0.93      0.95       319\n",
      "faulty_generalization       0.96      0.94      0.95       449\n",
      "                 none       0.97      0.99      0.98      1670\n",
      "\n",
      "             accuracy                           0.98      3500\n",
      "            macro avg       0.98      0.97      0.97      3500\n",
      "         weighted avg       0.98      0.98      0.98      3500\n",
      "\n",
      "[[ 324    0    2    0    4    1]\n",
      " [   0  225    0    0    0    2]\n",
      " [   3    0  490    2    2    7]\n",
      " [   2    0    1  296    6   14]\n",
      " [   0    1    4    0  422   22]\n",
      " [   2    1    2    4    5 1656]]\n",
      "Multiclass Brier score: 0.040556967015328314\n"
     ]
    }
   ],
   "source": [
    "logger.info('predict on train_dataset')\n",
    "train_output = trainer.predict(train_dataset)\n",
    "\n",
    "classification_report, brier= get_eval_metrics(train_output, le)\n",
    "log_metrics(classification_report, brier, \"train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run funny-mouse-162 at: http://127.0.0.1:5001/#/experiments/445989666823303942/runs/c6af1fede7ef45b1aa5248d23633d585\n",
      "🧪 View experiment at: http://127.0.0.1:5001/#/experiments/445989666823303942\n"
     ]
    }
   ],
   "source": [
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save with pytorch\n",
    "mlflow.pytorch.save_model(model, path=SAVE_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
