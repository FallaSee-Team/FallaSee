{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f94bd545",
   "metadata": {},
   "source": [
    "# q3fer/distilbert-base-fallacy-classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1519430a",
   "metadata": {},
   "source": [
    "Source: https://huggingface.co/q3fer/distilbert-base-fallacy-classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cda090",
   "metadata": {},
   "source": [
    "### Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27790c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/katharinabaumgartner/Documents/NeueFische/scripts/34_capstone/Capstone_project/backend/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, TextClassificationPipeline \n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, classification_report, confusion_matrix\n",
    "import mlflow\n",
    "from mlflow.transformers import log_model\n",
    "import logging \n",
    "from logging import getLogger\n",
    "from transformers import AutoConfig\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TextClassificationPipeline\n",
    "import torch\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "#  import pickle\n",
    "import warnings # why? \n",
    "from mlflow.sklearn import save_model \n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # This tells Hugging Face: “Don’t use parallel tokenization — avoid possible deadlocks.”\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import config "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d73bdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from basic_functions import (\n",
    "    get_eval_metrics,\n",
    "    createTrainer, \n",
    "    get_encode_tokenize_data\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee9709ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"q3fer/distilbert-base-fallacy-classification\" # pulls the fallacy trained model\n",
    "TRACKING_URI = config.TRACKING_URI\n",
    "EXPERIMENT_NAME = config.EXPERIMENT_NAME\n",
    "configuration = AutoConfig.from_pretrained(MODEL_NAME)\n",
    "\n",
    "logging.basicConfig(format=\"%(asctime)s: %(message)s\") # Configure logging format to show timestamp before every message\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO) # Only show logs that are INFO or more important (e.g., WARNING, ERROR) — but ignore DEBUG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aeba3e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Label ID to Label Mapping:\n",
      "{0: 'ad hominem', 1: 'ad populum', 2: 'appeal to emotion', 3: 'circular reasoning', 4: 'equivocation', 5: 'fallacy of credibility', 6: 'fallacy of extension', 7: 'fallacy of logic', 8: 'fallacy of relevance', 9: 'false causality', 10: 'false dilemma', 11: 'faulty generalization', 12: 'intentional', 13: 'miscellaneous'}\n",
      "\n",
      "Reverse Mapping (Label to ID):\n",
      "{'ad hominem': 0, 'ad populum': 1, 'appeal to emotion': 2, 'circular reasoning': 3, 'equivocation': 4, 'fallacy of credibility': 5, 'fallacy of extension': 6, 'fallacy of logic': 7, 'fallacy of relevance': 8, 'false causality': 9, 'false dilemma': 10, 'faulty generalization': 11, 'intentional': 12, 'miscellaneous': 13}\n"
     ]
    }
   ],
   "source": [
    "# Load model config to inspect label mappings\n",
    "\n",
    "print(\"Model Label ID to Label Mapping:\")\n",
    "print(configuration.id2label)\n",
    "\n",
    "print(\"\\nReverse Mapping (Label to ID):\")\n",
    "print(configuration.label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d142a532",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"../data/data_dropped_duplicates_small.csv\"\n",
    "MODEL_PATH = \"q3fer/distilbert-base-fallacy-classification\"\n",
    "MODEL_TRAINING_PATH =\"q3fer/distilbert-base-fallacy-classification\"\n",
    "OUTPUT_DIR = \"../models/q3fer/distilbert-base-fallacy-classification/trainer_output\"\n",
    "SAVE_PATH = \"../models/q3fer/distilbert-base-fallacy-classification/pytorch_model\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f31bd6",
   "metadata": {},
   "source": [
    "### Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb1d16ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:basic_functions:Loading data...\n",
      "INFO:basic_functions:Train test split, test-size 0.3\n",
      "INFO:root:encode the label column\n",
      "INFO:root:tokenize\n",
      "INFO:basic_functions:create tokenizer & load model\n",
      "INFO:basic_functions:create tokenizer & load model\n",
      "INFO:root:create TextDatasets (train & test)\n"
     ]
    }
   ],
   "source": [
    "train_dataset, test_dataset, y_train, le = get_encode_tokenize_data(DATA_PATH, MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0740229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define new labels\n",
    "id2label = {\n",
    "    0: \"ad_hominem\",\n",
    "    1: \"appeal_to_authority\",\n",
    "    2: \"appeal_to_emotion\",\n",
    "    3: \"false_dilemma\",\n",
    "    4: \"faulty_generalization\",\n",
    "    5: \"none\"\n",
    "}\n",
    "\n",
    "label2id = {v: k for k, v in id2label.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd46821",
   "metadata": {},
   "source": [
    "### Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90c2c16d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = len(np.unique(y_train))\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "981718bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at q3fer/distilbert-base-fallacy-classification and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([14, 768]) in the checkpoint and torch.Size([6, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([14]) in the checkpoint and torch.Size([6]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME, \n",
    "    num_labels=num_classes,\n",
    "    ignore_mismatched_sizes= True\n",
    "    )\n",
    "model.config.id2label = id2label\n",
    "model.config.label2id = label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52e23fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 768])\n"
     ]
    }
   ],
   "source": [
    "final_layer = model.classifier\n",
    "# Check the size of the weights\n",
    "print(final_layer.weight.shape)  # Will be [num_labels, hidden_size] (num_labels x hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b03955f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBertConfig {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"ad_hominem\",\n",
      "    \"1\": \"appeal_to_authority\",\n",
      "    \"2\": \"appeal_to_emotion\",\n",
      "    \"3\": \"false_dilemma\",\n",
      "    \"4\": \"faulty_generalization\",\n",
      "    \"5\": \"none\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"ad_hominem\": 0,\n",
      "    \"appeal_to_authority\": 1,\n",
      "    \"appeal_to_emotion\": 2,\n",
      "    \"false_dilemma\": 3,\n",
      "    \"faulty_generalization\": 4,\n",
      "    \"none\": 5\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.50.3\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To see a summary of the model\n",
    "print(model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7558dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Active run_id: 6ee8680208c14afd806411fff81af478\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "      \"learning_rate\": 5e-5,\n",
    "      \"weight_decay\": 0.01,\n",
    "      \"num_train_epochs\": 3,\n",
    "      \"evaluation_strategy\": \"epoch\",\n",
    "      \"class_weight\":True,\n",
    "  }\n",
    "\n",
    "\n",
    "# setting the MLFlow connection and experiment\n",
    "mlflow.set_tracking_uri(TRACKING_URI)\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "\n",
    "mlflow.start_run()\n",
    "run = mlflow.active_run()\n",
    "print(\"Active run_id: {}\".format(run.info.run_id))\n",
    "\n",
    "mlflow.set_tag(\"model_name\", MODEL_NAME)\n",
    "mlflow.log_params(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37ca0d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:defining training arguments\n",
      "INFO:root:get weighted loss trainer\n"
     ]
    }
   ],
   "source": [
    "trainer = createTrainer(\n",
    "    model= model, \n",
    "    train_dataset = train_dataset,\n",
    "    test_dataset = test_dataset,\n",
    "    output_dir= OUTPUT_DIR, \n",
    "    y_train=y_train, \n",
    "    class_weight=True, \n",
    "    epochs=3, \n",
    "    learning_rate=5e-5, \n",
    "    weight_decay = 0.01, \n",
    "    train_batch_size=4, \n",
    "    eval_batch_size=8 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4271cb9a",
   "metadata": {},
   "source": [
    "### Execute Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "704a323d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mps.empty_cache()  # Clears unused GPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cf6c499e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# disable upper limit for memory\n",
    "os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.0\"\n",
    "\n",
    "# Allows up to 100% of available memory\n",
    "torch.mps.set_per_process_memory_fraction(1.0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "137c00ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:training is running\n",
      "2025/04/07 13:45:54 ERROR mlflow.utils.async_logging.async_logging_queue: Run Id 6ee8680208c14afd806411fff81af478: Failed to log run data: Exception: Changing param values is not allowed. Param with key='evaluation_strategy' was already logged with value='epoch' for run ID='6ee8680208c14afd806411fff81af478'. Attempted logging new value 'None'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2625' max='2625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2625/2625 57:02, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.219800</td>\n",
       "      <td>1.021656</td>\n",
       "      <td>0.657333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.769600</td>\n",
       "      <td>1.248680</td>\n",
       "      <td>0.719333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.260900</td>\n",
       "      <td>1.553619</td>\n",
       "      <td>0.732667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2625, training_loss=0.7991413915724982, metrics={'train_runtime': 3425.5757, 'train_samples_per_second': 3.065, 'train_steps_per_second': 0.766, 'total_flos': 1391006905344000.0, 'train_loss': 0.7991413915724982, 'epoch': 3.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logger.info('training is running')\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edda4f01",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fe7ab1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_metrics(cr, brier, split):\n",
    "    mlflow.log_metric(f\"{split}_brier\", brier)\n",
    "\n",
    "    for key, value in cr.items():\n",
    "        if (key == \"accuracy\"):\n",
    "                # print(f\"{split}_{key}\", round(value,2))\n",
    "                mlflow.log_metric(f\"{split}_{key}\", value)\n",
    "        else:\n",
    "            for metric in value:\n",
    "                mlflow.log_metric(f\"{split}_{key}_{metric}\", value.get(metric))\n",
    "                # print(f\"{split}_{key}_{metric}\", round(value.get(metric),2))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "456b6128",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:predict on train_dataset\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:basic_functions:get evaluation metrics\n",
      "INFO:basic_functions:classification_report\n",
      "INFO:basic_functions:confusion_matrix\n",
      "INFO:basic_functions:brier score\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       precision    recall  f1-score   support\n",
      "\n",
      "           ad_hominem       0.94      0.70      0.80       325\n",
      "  appeal_to_authority       0.57      0.86      0.69       229\n",
      "    appeal_to_emotion       0.72      0.82      0.77       487\n",
      "        false_dilemma       0.73      0.68      0.70       299\n",
      "faulty_generalization       0.67      0.78      0.72       445\n",
      "                 none       0.86      0.78      0.82      1715\n",
      "\n",
      "             accuracy                           0.78      3500\n",
      "            macro avg       0.75      0.77      0.75      3500\n",
      "         weighted avg       0.79      0.78      0.78      3500\n",
      "\n",
      "[[ 227   17   33    4    5   39]\n",
      " [   0  196   10    3    5   15]\n",
      " [   0   14  401    8   15   49]\n",
      " [   2    3   13  202   11   68]\n",
      " [   3   12   23    7  348   52]\n",
      " [  10  101   74   51  136 1343]]\n",
      "Multiclass Brier score: 0.3871642602247193\n"
     ]
    }
   ],
   "source": [
    "logger.info('predict on train_dataset')\n",
    "train_output = trainer.predict(train_dataset)\n",
    "\n",
    "classification_report, brier= get_eval_metrics(train_output, le)\n",
    "log_metrics(classification_report, brier, \"train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "de55b115",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:predict on test_dataset\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:basic_functions:get evaluation metrics\n",
      "INFO:basic_functions:classification_report\n",
      "INFO:basic_functions:confusion_matrix\n",
      "INFO:basic_functions:brier score\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       precision    recall  f1-score   support\n",
      "\n",
      "           ad_hominem       0.76      0.52      0.62       140\n",
      "  appeal_to_authority       0.37      0.67      0.48        98\n",
      "    appeal_to_emotion       0.63      0.73      0.67       208\n",
      "        false_dilemma       0.69      0.66      0.67       128\n",
      "faulty_generalization       0.48      0.50      0.49       191\n",
      "                 none       0.78      0.70      0.74       735\n",
      "\n",
      "             accuracy                           0.66      1500\n",
      "            macro avg       0.62      0.63      0.61      1500\n",
      "         weighted avg       0.68      0.66      0.66      1500\n",
      "\n",
      "[[ 73  18  23   4   4  18]\n",
      " [  0  66  12   0   2  18]\n",
      " [  3  12 151   3  13  26]\n",
      " [  0   3   8  84   2  31]\n",
      " [  5  12  18   5  96  55]\n",
      " [ 15  66  29  25  84 516]]\n",
      "Multiclass Brier score: 0.5168321446831328\n"
     ]
    }
   ],
   "source": [
    "logger.info('predict on test_dataset')\n",
    "test_output = trainer.predict(test_dataset)\n",
    "\n",
    "classification_report, brier = get_eval_metrics(test_output, le)\n",
    "log_metrics(classification_report, brier, \"test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d33e6787",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e808b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Add prediction and reporting here\n",
    "# output = trainer.predict(dataset[\"test\"])\n",
    "# predictions = np.argmax(output.predictions, axis=1)\n",
    "# y_true = output.label_ids\n",
    "\n",
    "# # Classification report\n",
    "# label_names = le.classes_\n",
    "# print(classification_report(y_true, predictions, target_names=label_names))\n",
    "\n",
    "# # Confusion matrix\n",
    "# cm = confusion_matrix(y_true, predictions)\n",
    "# print(\"Confusion Matrix:\")\n",
    "# print(cm)\n",
    "\n",
    "# # plotting\n",
    "# plt.figure(figsize=(10, 8))\n",
    "# sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=label_names, yticklabels=label_names)\n",
    "# plt.xlabel(\"Predicted\")\n",
    "# plt.ylabel(\"True\")\n",
    "# plt.title(\"Confusion Matrix\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1f7e9f",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291d4317",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save with pytorch\n",
    "mlflow.pytorch.save_model(model, path=SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ff82fd",
   "metadata": {},
   "source": [
    "### Checking the q3fer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a61895a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run inference with q3fer model on sample texts\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# pipeline = TextClassificationPipeline(model=model, tokenizer=tokenizer)\n",
    "\n",
    "# examples = [\n",
    "#     \"You have no idea what you're talking about; you've only lived here for six months.\", # ad_hominem\n",
    "#     \"I read a book by a nutritionist who says all carbs are bad.\", # appeal_to_authority\n",
    "#     \"Can I have the last piece of cake? You know how much I love it, and it's been a tough day for me.\", # appeal_to_emotion\n",
    "#     \"If we don't order pizza for dinner, we'll have to eat the week-old spaghetti in the fridge.\", # false_dilemma\n",
    "#     \"I was in Greece for two week. Greeks are amazing people!\", # faulty_generalization\n",
    "#     \"We should look into the science that supports this idea.\" # none\n",
    "# ]\n",
    "\n",
    "# predictions = pipeline(examples)\n",
    "\n",
    "# for i, pred in enumerate(predictions):\n",
    "#     print(f\"Text {i+1}: {examples[i]}\")\n",
    "#     print(f\"Prediction: {pred['label']} (Score: {pred['score']:.2f})\")\n",
    "#     print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4803b7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_mapping = {\n",
    "#     'ad hominem': 'ad_hominem',\n",
    "#     'appeal to emotion': 'appeal_to_emotion',\n",
    "#     'false dilemma': 'false_dilemma',\n",
    "#     'faulty generalization': 'faulty_generalization',\n",
    "#     'circular reasoning': 'other',\n",
    "#     'appeal to authority': 'appeal_to_authority',  \n",
    "#     'miscellaneous': 'other',  \n",
    "#     'fallacy of logic': 'other',\n",
    "#     'intentional': 'other',\n",
    "#     'ad populum': 'other',\n",
    "#     'equivocation': 'other',\n",
    "#     'false causality': 'other',\n",
    "#     'fallacy of relevance': 'other',\n",
    "#     'fallacy of credibility': 'other',\n",
    "#     'fallacy of extension': 'other'\n",
    "# }\n",
    "\n",
    "# #  the model's label doesn't include 'none'.Maybe we should/could test it out on our dataset with only the fallacies?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5c133d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # testing with the examples\n",
    "# mapped_predictions = []\n",
    "\n",
    "# for i, pred in enumerate(predictions):\n",
    "#     original_label = pred['label']\n",
    "#     mapped_label = label_mapping.get(original_label, 'unmapped')\n",
    "\n",
    "#     print(f\"Text {i+1}: {examples[i]}\")\n",
    "#     print(f\"Original Prediction: {original_label} (Score: {pred['score']:.2f})\")\n",
    "#     print(f\"Mapped to: {mapped_label}\")\n",
    "#     print(\"-\" * 50)\n",
    "\n",
    "#     mapped_predictions.append(mapped_label)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
